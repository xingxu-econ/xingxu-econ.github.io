[
  {
    "objectID": "Julia/Lectures/Julia_Lec_4_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_4_Compecon_Xing_Xu.html",
    "title": "Lecture 4: The Aiyagari Model",
    "section": "",
    "text": "In this lecture, I will introduce the basic set-up of the workhorse incomplete market model – the Aiyagari (1996) model and discuss how to compute it numerically. When writing this lecture, I largely follow from my answer for problem set 3 and 4 for Manuel Amador’s Macro class.\nUseful notes: Jesus Fernandez-Villaverde’s notes on heterogeneous agent models: part 1, part 2, part 3.\n\n1.General set up of the Aiyagari model\n\n1.1 Environment:\n\nHousehold:\nContinuum of household with mass 1:\nHousehold has identical preferences: \\[\n\\sum_{t=0}^\\infty \\beta^t u(c_t)\n\\],\nsubject to a budget constraint: \\(c_t + a_{t+1} \\leq w_t l_t(s^t) + R_t a_t, \\forall t\\),\nand a borrowing limit: \\(a_{t + 1} \\geq \\phi, \\forall t\\),\nwith \\(a_0, l_0 \\geq 0\\) given.\n\nLet labor endowments for each household follow a markov chain with a transition matrix \\(\\pi\\), \\(\\pi(s'|s) &gt; 0, \\forall s, s'\\). The invariant distribution of labor supply is a probability distribution \\(\\lambda\\) such that \\(\\lambda \\pi = \\pi\\).\nBy law of large number, the aggregate labor supply \\(L = \\sum_{s \\in S} \\lambda(s) l(s)\\) does not depend on \\(s\\) (no aggregate uncertainty). This is a key feature of the problem.\n\n\n\nFirm:\nA perfectly competitive firm with Neo-classical technology: \\[\nY_t = F(K_t, L_t).\n\\]\n\n\nAggregate resource constraint:\n\\[\nC_t + K_{t+1} - (1 - \\delta) K_t = F(K_t, L)\n\\] with depreciation rate \\(\\delta \\in (0,1)\\).\n\n\n\n\n2. Recursive formulation of the household problem\nAssume utility is strictly increasing. Taking the interest rate \\(R\\) and \\(w\\) as given, the household problem takes the recursive form:\n\\(V(a, s) = \\max_{a'\\geq \\phi} u(Ra + w l(s) - a') + \\beta E[V(a', s')]\\)\nwhere \\(s\\) is the realization of the labor endowment today.\n\n2.1 Side notes on alternative specification of the household problem:\nAlternatively we can write the problem as \\(V(x, s) = \\max_{a'\\geq \\phi} u(x - a') + \\beta E[V(Ra' + w l(s'), s')]\\), where \\(x \\equiv Ra + w l(s)\\) is the cash in hand today. Note that the value function will be different as it is now a function of \\(x\\). The key here is that one need to make sure that the state variables on both sides are consistent.\nOn top of this, Aiyagari used a trick to simplify the problem. Define \\(\\hat{a} = a + \\phi\\), \\(\\hat{x} = x + \\phi\\). Then, the household problem can be rewritten as:\n\\(v(\\hat{x}, s) = \\max_{\\hat{a}' \\geq 0} u(\\hat{x} - \\hat{a}') + \\beta E[v(R\\hat{a}' + w l(s') - (R - 1) \\phi), s']\\)\n\nNow let’s try to solve the household problem using \\(a\\) as the first state variable given particular \\(R\\) and \\(w\\). This is essentially a partial equilibrium and is often referred to as a Bewley/Hugget model.\n\n\n2.2 Computing the household problem\nNote that here only the product of \\(w\\) and \\(l\\) matters here and there’s no interesting mechanism for determining \\(w\\). For simplicity, we define \\(y(s) \\equiv wl(s)\\) such that \\(V(a, s) = \\max_{a'\\geq \\phi} u(Ra + y(s) - a') + \\beta E[V(a', s')]\\). Further assume \\(s\\) follows a first-order markov chain governed by states \\(y(s)\\) and transition matrix \\(P\\).\nWe will construct grids using \\(a\\). Let \\(a\\) start from \\(\\phi\\) so the agent cannot over-borrow.\nThe following codes are adopted from problem set 2 and 3 from Manuel Amador’s Macro class.\nAs before, Base.@kwdef struct is used to store the parameters. However here instead of some specific types like Float64, we use parametric types like R1 and T1. These can really be any types dependent on the input. Check the documentation for parametric types here.\n\nusing LinearAlgebra\nusing Plots\n\n\nBase.@kwdef struct Household{T1, T2, T3, R1, S}\n    β::R1 = 0.95\n    ρ::R1 = 2.0 \n    ϕ::R1 = 0.0  # The borrowing limit is zero \n    P::T1 = [0.5 0.5; 0.2 0.8] # transition matrix for state s\n    l::T2 = [0.5, 1.0] # labor endowment: l(s) (normalize w to 1.0 so that y(s) = l(s))\n    a_min::R1 = ϕ\n    a_max::R1 = 5.0\n    points::S = 10_000\n    \n    a_grid::T3 = range(a_min, a_max, points)\nend\n\nu(c, m) = c ^ (1 - m.ρ) / (1 - m.ρ)  \nuprime(c, m) = c ^ (-m.ρ) \n\nuprime (generic function with 1 method)\n\n\nNow solve the value function as before. Since we have two states now, we need a matrix to store the value and policy for each different combination of \\(a\\) and \\(y\\).\nThere are mainly two tricks in the code. The first is to pre-compute the expected discounted value for each \\(a'\\) and \\(s'\\) conditional on \\(s\\) within the outer loop. The second is to take use of the fact of strictly increasing policy function as we have seen in Lecture 2.\n\nfunction solve_household(h; R = 0.9, w = 1.0, v0 = zeros(length(h.a_grid), length(h.l)),  tol = 1e-6)\n    \n    # unpacking \n    (; a_grid, l, β, P) = h\n    \n    v1 = similar(v0)\n    pol = similar(v0, Int)\n    βv = similar(v1)\n    \n    n = length(a_grid)\n    \n    iter = 0\n    while true\n        distance = zero(eltype(v0))\n        iter += 1 \n        \n        # Precompute βE[V(a', s')] for better performance\n        for s in eachindex(l)\n            for i in eachindex(a_grid)\n                accum = zero(eltype(v0))\n                for sprime in eachindex(l)\n                    accum += β * P[s, sprime] * v0[i, sprime]\n                end\n                βv[i, s] = accum\n            end \n        end\n        # or simply βv = β .* v0 * P' \n\n        for s in eachindex(l) \n            pol_i = 1\n            for i in eachindex(a_grid)\n                just_started = true \n                vmax = zero(eltype(v0))\n                for j in pol_i:n #take use of strictly increasing policy function\n                    c = R * a_grid[i] + w*l[s] - a_grid[j]\n                    if c &gt; 0.0\n                        v_tmp = u(c, h) + βv[j, s]\n                        if just_started\n                            vmax = v_tmp\n                            pol_i = j\n                            just_started = false\n                        elseif v_tmp &gt; vmax \n                            vmax = v_tmp \n                            pol_i = j\n                        else \n                            break \n                        end \n                    end \n                end \n                v1[i, s] = vmax\n                pol[i, s] = pol_i\n                dis = abs(vmax - v0[i, s])\n                if dis &gt; distance\n                    distance = dis\n                end \n            end \n        end \n        \n        if distance &lt; tol\n            break\n        else \n            v0 .= v1\n        end \n    end \n    return (v = v1, pol = pol, h = h, R = R, w = w)\nend \n    \n\nsolve_household (generic function with 1 method)\n\n\nExplanations over how the code works (GPT generated, which is actually pretty good):\nThis function solve_household solves the household problem defined earlier using value function iteration. The function takes in the model parameters as input, par, and some optional arguments such as an initial guess for the value function v0 and a tolerance level tol.\nThe function initializes some variables such as v1 and pol as arrays of zeros with the same shape as v0, which is the value function from the previous iteration. βv is also initialized as an array of zeros, which will be used to store the expected value of the next iteration.\nThe function then performs the value function iteration using a nested loop over ytilde and ahat_grid. The inner loop first computes the expected value of the next iteration using the Bellman equation and stores it in βv.\nThe inner loop then maximizes the Bellman equation over the choice of next period asset holding ahat_grid[j] using a nested loop that starts at pol_i. This is done to take advantage of the fact that the optimal choice of ahat_grid[j] is increasing in j. The loop computes the value of the Bellman equation for each ahat_grid[j] and stops once the first ahat_grid[j] is found that yields a negative consumption level c. The function then stores the optimal value function vmax and the corresponding choice of j in pol_i and moves on to the next ahat_grid[i].\nThe function then computes the distance between the value function from the current iteration v0 and the value function from the previous iteration v1. If the distance is less than the tolerance level tol, the function exits the loop and returns the value function v1, the policy function pol, and the model parameters m.\nOverall, the function uses a nested loop to solve the household problem using value function iteration, and it takes advantage of the fact that the problem has a recursive structure and a simple functional form for the utility function to compute the optimal value and policy functions efficiently.\n\nh = Household() # struct with default parameter values\nsol_1 = solve_household(h, R = 0.658) #input some random R. \n\n(v = [-26.71326843693425 -25.314667038332857; -26.711953170852723 -25.314338113656607; … ; -24.019524603316466 -23.72455416253825; -24.019409824729976 -23.72445885993552], pol = [1 1; 1 1; … ; 4194 4864; 4194 4864], h = Household{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.95, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [0.5, 1.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0), R = 0.658, w = 1.0)\n\n\nLet’s take a look at the value function and policy functions\n\nfunction do_v_plot(sol_1)\n\n    income = sol_1.w * sol_1.h.l \n    p1 = Plots.plot(xlabel = \"Current saving, a\", ylabel = \"Value, v\")\n    for i in eachindex(income)\n        temp = income[i]\n        plot!(p1, sol_1.h.a_grid, sol_1.v[:,i], label = \"y = $temp\")\n    end\n    \n    return p1\nend\n\ndo_v_plot(sol_1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction do_pol_plot(sol_1)\n\n    (; pol, h)= sol_1 \n\n    p1 = Plots.plot(xlabel = \"Current saving, a\", ylabel = \"Saving next period, a' \")\n    for s in eachindex(h.l)\n        temp = sol_1.w * h.l[s]\n        yvals = [[h.a_grid[pol[i, s]] for i in eachindex(h.a_grid)]]\n        plot!(p1, sol_1.h.a_grid, yvals, label = \"y = $temp\")\n    end\n    \n    return p1\nend\n\ndo_pol_plot(sol_1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that when \\(y\\) is low. The policy function for \\(a'\\) is flat. The intuition is that the agent has incentive to borrow for consumption smoothing when endowment is low (why?) but is borrowing constrained due to the \\(0\\) borrowing limit. As a result, the agent consumes all her endowment. This is sometimes referred to as “Hand-to-mouth” consumption.\n\n\n\n3. Recursive competitive equilibrium (Aiyagari 1994)\nWith some understanding of the partial equilibrium, we now try to build towards the general equilibrium. With household optimality, firm optimality and market clearing conditions, the equilibrium can be formally defined by a recursive competitive equilibrium.\nThe key difference is that every household now needs to know the distribution of everyone else’s asset holdings to infer future interest rate.\nLet \\(a \\in A \\equiv [\\phi, \\infty]\\), \\(s \\in S\\). From the recursive form of the household problem, for given R we can solve for a value function \\(v(a, s): A \\times S \\to R\\) and a policy function for saving \\(g(a, s): A \\times S \\to A\\).\n\n3.1 Prices in general equilibrium\nNow for simplicity, let the firm technology be characterized by a CRS Cobb-Douglas production function, i.e. \\[\nY_t = A K_t^\\alpha L_t^{1 - \\alpha}.\n\\]\nThe firm’s problem takes the form: (why?)\n\\[\\max_{K_t, L_t} A K_t^\\alpha L_t^{1 - \\alpha} - (R_t -1 +\\delta) K_t - w_t L_t\\]\nFirm’s first order conditions: \\[\n\\begin{cases}\nR_t = A \\alpha K_t^{\\alpha - 1} L_t^{1 - \\alpha} +1 - \\delta \\\\\nw_t = A (1 - \\alpha) K_t^\\alpha L_t^{-\\alpha}\n\\end{cases}\n\\]\nNote that \\(w\\) can be pinned down by \\(R\\) in equilibrium: \\[\nw_t(R_t) = A^{\\frac{1}{1-\\alpha}} (1 - \\alpha) \\alpha^{\\frac{\\alpha}{1 - \\alpha}} \\left(R_t - 1 +\\delta \\right)^{\\frac{\\alpha}{1 - \\alpha}}\n\\]\nThis is very handy when it comes to computing the equilibrium prices.\n\n\n3.2 Set up of a (recursive) stationary competitive equilibrium\nA stationary competitive equilibrium is defined by a system of constant prices (interest rate and wage) and allocations such that individuals optimize and markets clear.\nDefine \\(\\lambda_t(a, s): A \\times S \\to [0,1]\\) to be the distribution of household over current asset \\(a\\) and state \\(s\\). Imagine it to be a matrix with each entry filled with the fraction of household with each combination of possible \\(a\\) and \\(s\\). Hypothetically, this should evolve over time following a law of motion: \\[\n\\lambda_{t+1}(a', s') = \\sum_{s \\in S} \\sum_{a: g(a,s) = a'} \\pi(s'|s) \\lambda_t(a,s), \\quad \\forall a', s'\n\\]\nwhere \\(\\pi(s'|s)\\) is the transition probability and \\(\\sum_{a: g(a,s) = a'}\\) reads that all \\(a\\) such that \\(g(a,s) = a'\\). Intuitively, the fraction of households that falls into the state \\(a'\\) and \\(s'\\) next period will be the fraction of households (with different \\(a\\) and \\(s\\)) that choose \\(a'\\) times the conditional probability that the household moves into \\(s'\\) next period.\nDoes the economy converge to some kind of steady state (stationary equilibrium)? Here, in a stationary equilibrium, \\(\\lambda_t = \\lambda, \\forall t\\) and all aggregate real quantities and prices stay constant. Aiyari (1994) proved that under some conditions, with no aggregate shock, a stationary competitive equilibrium exists for the economy. For clarity, I will formally define the stationary competitive equilibrium as follows.\nA stationary competitive equilibrium is scalars \\(\\{R, w, K, L\\}\\) and functions \\(\\{v, g, \\lambda\\}\\) such that:\n\nThe value function, \\(v\\) satisfies the Household’s problem given \\(R, w\\) and \\(l(s)\\), and \\(g\\) is an optimal policy to the problem.\n\\(\\lambda\\) is the stationary distribution that arises from \\(g\\) and \\(\\pi\\) (from above).\nAsset market clears, i.e. \\(K = \\sum_{s \\in S} \\sum_{a \\in A} \\lambda(a,s) g(a,s)\\).\nLabor market clears, i.e. \\(L = \\sum_{s \\in S} \\sum_{a \\in A} \\lambda(a,s) l(s)\\).\nFirm’s optimality conditions are satisfied.\n\nGood’s market will clear with a Walras’ Law-ish argument.\nNow let’s build towards computing the model. Add some more parameters in the struct.\n\nBase.@kwdef struct Household_GE{T1, T2, T3, R1, S}\n    β::R1 = 0.7\n    ρ::R1 = 2.0 \n    ϕ::R1 = 0.0  # The borrowing limit is zero \n    P::T1 = [0.5 0.5; 0.2 0.8] # transition matrix for state s\n    l::T2 = [1.0, 5.0] # l(s)\n    a_min::R1 = ϕ\n    a_max::R1 = 5.0\n    points::S = 10_000\n\n    a_grid::T3 = range(a_min, a_max, points)\n\n    A::R1 = 1.20\n    α::R1 = 0.7\n    δ::R1 = 1.0  #full depreciation\nend\n\nu(c, m) = c ^ (1 - m.ρ) / (1 - m.ρ)  \nuprime(c, m) = c ^ (-m.ρ) \n\nuprime (generic function with 1 method)\n\n\n\nh_GE = Household_GE()\n\nHousehold_GE{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.7, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [1.0, 5.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0, 1.2, 0.7, 1.0)\n\n\nIt is useful to note that at stationary equilibrium, \\(L\\) is a constant. This is because if you solve for the invariant distribution of \\(P\\) (the transition probability \\(\\pi\\) ), \\(L\\) can be written as \\(\\sum_{s \\in S} \\pi(s) l(s)\\). Here we will numerically compute \\(L\\).\n\nfunction compute_L(P, l)\n    # approximate the invariant distribution of labor endowment (inefficient)\n    temp = P^100\n    L = 0\n    for i in eachindex(l)\n        L = L + l[i] * temp[1,i] \n    end\n    return L\nend\n\ncompute_L(h_GE.P, h_GE.l)\n\n3.857142857142877\n\n\nFirst let’s compute the invariant distribution given some \\(R\\). For simplicity let \\(A\\) be 1 (not the correct way if you want to calibrate the model). Since we want to include firm’s optimality conditions, modify the function for computing \\(v\\) to include firm’s optimality condition. Now we have \\(K\\) as input and then solve for \\(R\\) and \\(w\\) endogenously.\n\nfunction solve_household_GE(h_GE; K = 0.5, v0 = zeros(length(h_GE.a_grid), length(h_GE.l)),  tol = 1e-6)\n    \n    # unpacking \n    (; a_grid, l, β, P, A, α, δ) = h_GE\n    \n    L = compute_L(P, l)\n\n    R = A * α * K^(α - 1) * L^(1 - α) + 1 - δ# Get R from the firm's first order condition\n\n    w = A * (1 - α) * K^α * L^(- α) # Get w from the firm's first order condition\n\n\n    #Given K\n\n    v1 = similar(v0)\n    pol = similar(v0, Int)\n    βv = similar(v1)\n    \n    n = length(a_grid)\n    \n    iter = 0\n    while true\n        distance = zero(eltype(v0))\n        iter += 1 \n        \n        # Precompute βE[V(a', s')] for better performance\n        for s in eachindex(l)\n            for i in eachindex(a_grid)\n                accum = zero(eltype(v0))\n                for sprime in eachindex(l)\n                    accum += β * P[s, sprime] * v0[i, sprime]\n                end\n                βv[i, s] = accum\n            end \n        end\n\n        for s in eachindex(l) \n            pol_i = 1\n            for i in eachindex(a_grid)\n                just_started = true \n                vmax = zero(eltype(v0))\n                for j in pol_i:n #take use of strictly increasing policy function\n                    c = R * a_grid[i] + w * l[s] - a_grid[j]\n                    if c &gt; 0.0\n                        v_tmp = u(c, h_GE) + βv[j, s]\n                        if just_started\n                            vmax = v_tmp\n                            pol_i = j\n                            just_started = false\n                        elseif v_tmp &gt; vmax \n                            vmax = v_tmp \n                            pol_i = j\n                        else \n                            break \n                        end \n                    end \n                end \n                v1[i, s] = vmax\n                pol[i, s] = pol_i\n                dis = abs(vmax - v0[i, s])\n                if dis &gt; distance\n                    distance = dis\n                end \n            end \n        end \n        \n        if distance &lt; tol\n            break\n        else \n            v0 .= v1\n        end \n    end \n    return (v = v1, pol = pol, h = h_GE, R = R, w = w)\nend \n    \n\nsolve_household_GE (generic function with 1 method)\n\n\n\nsol_2 = solve_household_GE(h_GE, K = 0.75)\n\n(v = [-17.561774071593458 -7.641618063043207; -17.509638148903143 -7.637543522427641; … ; -1.5204371786915847 -1.4164179272473385; -1.5203069683874395 -1.4163050174215155], pol = [1 324; 1 325; … ; 9364 9940; 9365 9941], h = Household_GE{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.7, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [1.0, 5.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0, 1.2, 0.7, 1.0), R = 1.3729054349841805, w = 0.11440878624868113)\n\n\n\nfunction compute_stationary_distribution(sol; tol = 1e-8, pdf_0 = fill(1.0 / prod(size(sol.v)), size(sol.v)) )\n    (; pol, h) = sol\n    \n    pdf_1 = similar(pdf_0)\n    \n    while true \n        fill!(pdf_1, zero(eltype(pdf_0)))\n        \n        for i in eachindex(h.a_grid)\n            for s in eachindex(h.l)\n                for sprime in eachindex(h.l)\n                    pdf_1[pol[i, s], sprime] += h.P[s, sprime] * pdf_0[i, s]\n                end\n            end \n        end\n        \n        distance = zero(eltype(pdf_0))\n        for (a, b) in zip(pdf_0, pdf_1)\n            distance = max(abs(a - b), distance)\n        end\n        \n        (distance &lt; tol) && break \n        pdf_0 .= pdf_1\n    end \n    return pdf_1\nend \n\ncompute_stationary_distribution (generic function with 1 method)\n\n\n\npdfA = compute_stationary_distribution(sol_2)\nplot(sol_2.h.a_grid, sum(pdfA, dims = 2)[:, 1], label = \"pdf\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe PDF is not super smooth due to computational errors. You can try adding more grids (doesn’t work very well), but the better method is to use continuous optimization. Keep in mind the numerical errors from here. It will come back later.\nOf course you can also plot the CDF.\n\nplot(sol_2.h.a_grid, cumsum(sum(pdfA, dims = 2)[:, 1]), label = \"cdf\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also compute the aggregate asset supply under the invariant distribution, which is simply:\n\n# function for computing A under the invariant distribution\nfunction compute_A(sol, pdf)\n    a = similar(sol.v)\n    a = sol.h.a_grid[sol.pol]\n    return a⋅pdf \nend\n\ncompute_A (generic function with 1 method)\n\n\n\n@show compute_A(sol_2, pdfA)\n\ncompute_A(sol_2, pdfA) = 1.047829596172126\n\n\n1.047829596172126\n\n\n\n\n3.3 The capital demand and supply curve\nSince the aggregate labor supply \\(L\\) is a constant as we have argued, the only equilibrium object that has to been determined endogenously is the market clearing capital (or asset). Now, the equilibrium interest rate \\(R = AαK^{α - 1} L^{1 - α} + 1 - δ\\) is a one-to-one function of \\(K\\). The inverse function \\(K(R)\\) gives the capital demand curve, which is continuous and downward sloping. From the household problem, given each R, we can endogenously solve out the asset supply curve with the invariant distribution of household, which will be continuous and upward sloping. I denote the supply as \\(A(R)\\). The unique intersection of \\(K(R)\\) and \\(A(R)\\) gives out the equilibrium capital and the corresponding interest rate.\nThe following function solves out the aggregate asset supply and capital demand over a grid of possible equilibrium interest rate \\(R\\).\n\nfunction K_demand_supply(h_GE; R_grids = range(0.95, 1.0/h_GE.β, 100), K_demand = similar(R_grids), A_supply = similar(R_grids), w_grids = similar(R_grids))\n    (; a_grid, l, β, P, A, α, δ) = h_GE\n    L = compute_L(P, l)\n    # get the demand function\n    K_demand  = @. ((R_grids  - 1 + δ)/(A * α  * L^(1 - α)))^(1/(α - 1))\n\n    # equilibrium wages\n    w_grids = @. A^(1/(1-α)) * (1 - α) * α^(α/(1 - α)) / (R_grids - 1 + δ)^(α/(1-α))\n    # get the supply function (use the original VFI function)\n    for i in eachindex(R_grids)\n        sol = solve_household(h_GE, R = R_grids[i], w = w_grids[i])\n        pdf = compute_stationary_distribution(sol)\n        A_supply[i] = compute_A(sol, pdf)\n    end\n    return R_grids, K_demand, A_supply\nend\n\nR_grids, K_demand, A_supply = K_demand_supply(h_GE)\n\n(0.95:0.004834054834054834:1.4285714285714286, [2.5592811230762695, 2.516345927319649, 2.474342418197531, 2.4332458368040855, 2.3930322016718937, 2.353678280645201, 2.3151615639038146, 2.277460238085242, 2.2405531614551712, 2.2044198400790145  …  0.7282763869637299, 0.719867466600011, 0.7115843300746376, 0.7034246696389707, 0.695386227650787, 0.6874667953188908, 0.6796642114833356, 0.6719763614301456, 0.6644011757394387, 0.6569366291659031], [0.3935551678730847, 0.39275776490277464, 0.39199031525591366, 0.3911147352032359, 0.39089303063609615, 0.39115584409938453, 0.39160472443612426, 0.39218564845568704, 0.3928700139055018, 0.39364469678411473  …  1.2179868137367416, 1.3090272943476338, 1.4205468127468952, 1.5608866788486613, 1.7443162211259264, 1.9941783002778637, 2.3511808837319235, 2.847716210221898, 3.433878827587586, 3.958195215474183])\n\n\n\nfunction K_plot(K_demand, A_supply, h_GE; R_grids = range(0.95, 1.0/h_GE.β, 100))\n    plot(R_grids, K_demand, label = \"K(R)\")\n    plot!(R_grids, A_supply, label = \"A(R)\")\nend\n\nK_plot(K_demand, A_supply, h_GE)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can look at when \\(A(R)\\) first exceeds \\(K(R)\\) to have a guess on the equilibrium capital and interest rate.\n\nfunction find_first(K_demand, A_supply)\n    idx = 0\n    for i in eachindex(K_demand)\n        if A_supply[i] &gt; K_demand[i]\n            idx = i\n            break\n        end\n    end\n    return idx\nend\n\ni = find_first(K_demand, A_supply)\n\nK1 = K_demand[i]\nK2 = K_demand[i - 1]\nR1 = R_grids[i - 1]\nR2 = R_grids[i]\n\nprint(\"The equilibrium capital is between $K1 and $K2. The equilibrium interest rate is between $R1 and $R2.\")\n\nThe equilibrium capital is between 0.8003704925496153 and 0.8100242546324644. The equilibrium interest rate is between 1.3415584415584416 and 1.3463924963924965.\n\n\nChallenge for solving the equilibrium:\n\n\n3.4 Solving the stationary competitive equilibrium\nNow we have everything ready to compute the stationary competitive equilibrium.\nThe algorithm goes as follows:\n\nStart from a given \\(K_0\\)\nFor each \\(K_t\\) and obtain \\(R\\) and \\(w\\)\nSolve the household problem given \\(R\\) and \\(w\\)\nCompute invariant distribution \\(\\lambda(a, s)\\)\nGet \\(A \\equiv \\sum_{s \\in S} \\sum_{a \\in A} \\lambda(a,s) g(a,s)\\).\nGuess \\(K_{t+1} = ϵ A + (1 - ϵ) K_t\\). Go back to the second step and iterate until convergence.\n\n\ncompute_A(sol_2, pdfA)\n\n1.047829596172126\n\n\n\nfunction compute_stationary_equilibrium(h_GE; K0 = 0.70, tol = 1e-5, iterate = 0, ϵ = 0.3)\n    sol = solve_household_GE(h_GE, K = K0)\n    pdf = compute_stationary_distribution(sol)\n    A = compute_A(sol, pdf)\n\n    K = K0\n    K1 = ϵ * A + (1 - ϵ) *  K\n\n    while true \n        distance = zero(eltype(K))\n        sol = solve_household_GE(h_GE, K = K)\n        pdf = compute_stationary_distribution(sol)\n        A = compute_A(sol, pdf)\n        K1 = ϵ * A + (1 - ϵ) * K # for speed\n\n        distance = abs(K1 - K)\n        (distance &lt; tol || iterate == 100) && break\n        iterate += 1\n        K = copy(K1)\n    end \n    return (K_rce = K, sol_rce = sol, iterate = iterate) \nend\n\ncompute_stationary_equilibrium (generic function with 1 method)\n\n\nWarning: this can get slow as the parameter values are casually set and the algorithms are not optimized. If you play around with the parameters you might notice that sometimes it doesn’t converge. I will discuss this with more detail.\n\n@time begin\n    sol_GE = compute_stationary_equilibrium(h_GE)\nend\n\n  0.706224 seconds (24.39 k allocations: 12.320 MiB, 2.37% compilation time)\n\n\n(K_rce = 0.807696820287375, sol_rce = (v = [-16.717896304757282 -7.336975047166834; -16.67191258870082 -7.3334192227477315; … ; -1.606486499114749 -1.490479825567775; -1.6063520123876334 -1.4903641858039804], pol = [1 337; 1 338; … ; 9198 9812; 9199 9813], h = Household_GE{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.7, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [1.0, 5.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0, 1.2, 0.7, 1.0), R = 1.342717011889535, w = 0.12050091789432643), iterate = 7)\n\n\n\n# equilibrium interest rate\n@show sol_GE.sol_rce.R\n\nsol_GE.sol_rce.R = 1.342717011889535\n\n\n1.342717011889535\n\n\n\n# equilibrium wage \n@show sol_GE.sol_rce.w\n\nsol_GE.sol_rce.w = 0.12050091789432643\n\n\n0.12050091789432643\n\n\n\nfunction do_pol_plot_GE(sol_1)\n\n    (; pol, h)= sol_1 \n\n    p1 = Plots.plot(xlabel = \"Current saving, a\", ylabel = \"Saving next period, a' \")\n    for s in eachindex(h.l)\n        temp = h.l[s]\n        yvals = [[h.a_grid[pol[i, s]] for i in eachindex(h.a_grid)]]\n        plot!(p1, sol_1.h.a_grid, yvals, label = \"l = $temp\")\n    end\n    \n    return p1\nend\n\n\ndo_pol_plot_GE(sol_GE.sol_rce)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5 What properties are important for solving the stationary equilibrium?\nOriginally the value that labor endowment process takes are 2 and 4, with same transition probability (now it’s 1 and 5). We can see that the algorithm for solving the stationary equilibrium doesn’t converge.\n\nh_GE1 = Household_GE(l = [2.0, 4.0])\n\nHousehold_GE{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.7, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [2.0, 4.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0, 1.2, 0.7, 1.0)\n\n\nLet’s look at the capital demand and supply curve. Do you notice anything different?\n\nR_grids1, K_demand1, A_supply1 = K_demand_supply(h_GE1)\nK_plot(K_demand1, A_supply1, h_GE1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main difference is steeper asset supply curve, especially close to where the equilibrium asset and interest rate lies. We can check if we can solve for the stationary equilibrium.\n\nsol_GE1 = compute_stationary_equilibrium(h_GE1)\n\n(K_rce = 0.689482173249647, sol_rce = (v = [-10.938373253202755 -8.118259358389057; -10.926025926801701 -8.114221970562886; … ; -1.5501109947084142 -1.4943803850038575; -1.5499807113138844 -1.4942593212902437], pol = [1 118; 1 119; … ; 9491 9788; 9492 9789], h = Household_GE{Matrix{Float64}, Vector{Float64}, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, Float64, Int64}(0.7, 2.0, 0.0, [0.5 0.5; 0.2 0.8], [2.0, 4.0], 0.0, 5.0, 10000, 0.0:0.0005000500050005:5.0, 1.2, 0.7, 1.0), R = 1.3591154017758917, w = 0.11713573011418807), iterate = 100)\n\n\n\n@show sol_GE1.iterate\n\nsol_GE1.iterate = 100\n\n\n100\n\n\nRecall that 100 is the upper bound we set for the total iterations. You can change it to higher value but in short you don’t get convergence. It is also often the case with other parameter choices as long as the asset supply curve is steep around where equilibrium lies around, i.e. the elasticity for interest rate is high.\nWhy this is troublesome? Now because the aggregate asset supply rises sharply where \\(R\\) is close to the equilibrium, to find an equilibrium we need good approximations of the policy function around where it matters. However recall that we used even grids for the assets, which causes inaccuracy of the policy function as we simply don’t have enough points where it truly matters. Again this is just due to approximation errors. A naive way to resolve it is to put more grids around the equilibrium point. A better alternative is to simply obtain better precisions with continuous optimization method (you are asked to perform this in the assignment 4). (For enthusiasts, another possible solution is endogenous grid methods).\nFor our purpose, we can just make the asset supply less elastic. That’s why I make the possible values of labor endowments wider. The reason this works goes back to the very nature of the “incompleteness” of the model. Household can’t insure against all future risks, so they have to save more than in a complete market due to strict concavity of utility. This is called precautionary saving motivation. When the labor endowments get more fluctuant, either through wider values or smaller autocorrelation, the household saves more due to this motive. Fixing other parameters, households’ saving behavior is more driven by the uncertainty of the income shock.\nIn general, we should try to obtain better precisions through continuous optimizations (good exercise) and other computational techniques. An important lesson here is that precision does matter and you should always keep that in mind.\nFor clarity, the key here lies in the comparison between the incentives to save due to precautionary saving and higher interest rate. Just for intuition, we enhanced the first channel and made the asset supply less elastic of the second.\nA final comment is that don’t be afraid when your code doesn’t work. Of course mostly it is some random mistakes and as long as you correct them, it is perfectly fine. More importantly, there are cases where something that doesn’t work actually teaches you a lesson, and often a most valuable one. Digging into some seemingly weird situations can provide valuable insights, as is the case here.\n\n\n\n4. Transition dynamics for incomplete market (extended reading)\nEconomists are often interested in the short-run response of agents given an unexpected one-time shock (often referred to as the MIT shock). Now we have the tools to compute the stationary competitive equilibrium. However as you can see, even under stationarity, this is not very simple.\nAssuming that households don’t have perfect foresight over prices, this implies that household needs to know everyone else’s state today (knowing the distribution is sufficient) to estimate the prices next period. In dynamic programming term, you need the whole distribution as an additional state variable. Note that the distribution is of very high dimensionality (here \\(A \\times s\\)), taking conditional expectation over this gigantic object is merely impossible.\nKrussel Smith (1998) is a very important contribution to the computation technique of the model. They are able to show that first order approximations over prices provide very accurate results because the policy function is close to linear when \\(a\\) is not very low.\nSome latest developments of the field is the sequence Jacobian. Auclert et al. provides a computation technique that is crazy fast and the purtabation method by Bhandari et al.\n\n\n5. Conclusion\nIncomplete market models are the workhorse models in modern Macroeconomic research, particularly in Macro-Labor and Macro-Finance. Knowing how to compute the basic model is of vital importance. In this lecture I started with the partial equilibrium version of the heteorgeneous household saving model (known as a Bewley/Hugget model), then I built towards the general equilibrium version of the model (known as an Aiyagari model). This lecture serves more as an introduction to allude readers to dive deeper themselves."
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_6_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_6_Compecon_Xing_Xu.html",
    "title": "Lecture 6: McCall’s Job search model",
    "section": "",
    "text": "The McCall’s job search model is a classic model and widely used as the introduction to dynamic programming due to its simplicity and flexibility. However for the consistency with Tim’s Macro class, I discuss the Neoclassical growth model first.\nIn this lecture, we will compute the basic model and add a bunch of extensions.\nMain reference:\n\nTim’s Notes\nQuantecon’s notes\n\n\n3.1 The basic setup (Largely follows from Tim’s notes)\nAn unemployed worker faces a wage offer \\(w\\) every period. We assume the cumulative distribution of the wage offer to be \\(F(v) = Prob(w \\leq v), w \\in [0, B]\\).\nThe unemployment can choose either to accept the offer and get \\(w\\) forever, or receive unemployment benefit \\(b\\) and search again.\nAn unemployed worker solves \\[\n\\max E \\sum_{t = 0}^\\infty \\beta^t y_t\n\\]\nwhere \\(y_t = \\begin{cases}\nw \\text{ if employed } \\\\\nb \\text{ if unemployed }\n\\end{cases}\\).\nThe Bellman equation for an unemployed worker is: \\[\nV(w) = \\max \\{ \\frac{w}{1 - \\beta}, b + \\beta EV(w') \\},\n\\] \\[\nV(w) = \\max \\{ \\frac{w}{1 - \\beta}, b + \\beta \\int_0^B V(w')dF(w') \\}\n\\]\n\nusing Pkg\nPkg.add([\"LaTeXStrings\", \"Statistics\", \"Distributions\", \"Expectations\"])\n\n\nusing LaTeXStrings # For Latex String in the graph\nusing LinearAlgebra, Statistics\nusing Distributions, Expectations\nusing Random\n\n\nusing StatsPlots\n\n\n\n3.2 Theoretical derivation\nSuppose we have solved the problem and found \\(V(w)\\). Then \\[\n\\bar{V} = b + \\beta \\int_0^B V(w')dF(w')\n\\] is a constant. Let \\(\\bar{w}\\) be such that \\[\n\\frac{\\bar{w}}{1 - \\beta} = \\bar{V} = b + \\beta \\int_0^B V(w')dF(w')\n\\]\nThen we have the policy function (as an indicator) being the unemployed worker will reject the offer if \\(w &lt; \\bar{w}\\) and accept the offer if \\(w \\geq \\bar{w}\\).\nSo hypothetically, the value function should look like:\n\nw = range(0, 4.0, length=100)\nv1 = 1/(1 - 0.7) .* w\nv2 = similar(w)\nv2 .= 7.98\nv3 = [max(v1[i], v2[i]) + 0.07 for i in 1:100] # +0.07 for asthetics\nplot(w, v1, line = (:red, 1), label = L\"\\frac{w}{1 - β}\")\nplot!(w, v2, line = (:green, 1), label = L\"\\bar{V}\")\nplot!(w, v3, line = (:blue, 2.5), label = L\"V(w)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor formal derivations of the result check Tim’s notes.\nNow for simplicity, let the wage offer follows a discrete distribution that is uniform across the grids (this mimics the behavior of a continuous uniform distribution).\n\n\n3.3 Computing the model (VFI)\nThe steps are similar to those of the second lecture.\n\n# struct of parameters\nBase.@kwdef struct worker# or simply @kwdef\n    β :: Float64 = 0.7\n    b :: Float64 = 1.5 # unemployment benefit\n    N :: Int64 = 100\n\n    w = range(0.0, 4.0, N) # range of the distribution of wage offer \n    P :: Vector{Float64} = ones(N)./N # probability distribution\nend\n\nw_parameter = worker()\n\nworker(0.7, 1.5, 100, 0.0:0.04040404040404041:4.0, [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01  …  0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])\n\n\n\nfunction Mccall_discrete(w_parameter; tol = 1e-6)\n     # unpacking \n    (; β, b, N, w, P) = w_parameter\n    \n    v = ones(N)\n    pol = similar(v, Int)\n    v1 = similar(v)\n    βEv = zero(eltype(v))\n\n    iterate = 0\n\n    while true  \n        distance = zero(eltype(v))\n        \n        βEv = β * P' * v \n\n        # not very efficient\n        for i in eachindex(v1)\n            if w[i]/(1 - β) ≥ b + βEv \n                v1[i] = w[i]/(1 - β)\n                pol[i] = 2 #accepting the offer\n            else\n                v1[i] = b + βEv\n                pol[i] = 1 #rejecting the offer\n            end\n        end\n        \n        distance = maximum(abs.(v1 - v))\n\n        (distance &lt; tol || iterate == 1000) && break # break out of the whole loop if one of the statements is true\n        iterate += 1\n\n        # Vectorized update of v using element-wise assignment\n        v .= v1\n    end\n\n    return (v=v, βEv = βEv, pol=pol, iterate=iterate)\n\nend\n\nMccall_discrete (generic function with 1 method)\n\n\n\nMccall_discrete(w_parameter)\n\n(v = [7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076, 7.758051209066076  …  12.12121212121212, 12.255892255892254, 12.39057239057239, 12.525252525252524, 12.659932659932657, 12.794612794612792, 12.929292929292927, 13.063973063973062, 13.198653198653197, 13.333333333333332], βEv = 6.258051619163654, pol = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  2, 2, 2, 2, 2, 2, 2, 2, 2, 2], iterate = 18)\n\n\n\nfunction v_plot_Mccall(w_parameter)\n    w = w_parameter.w\n    (v, βEv, pol, iterate) = Mccall_discrete(w_parameter)\n    v2 = similar(w)\n    v2.= βEv + w_parameter.b\n    plot(w, w/(1 -  w_parameter.β), line = (:red, 1), label = L\"\\frac{w}{1 - β}\")\n    plot!(w, v2, line = (:green, 1), label = L\"\\bar{V}\")\n    plot!(w, v .+ 0.1, line = (:blue, 2.5), label = L\"V(w)\")\nend\n\nv_plot_Mccall(w_parameter)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the graph, one can observe that with the specified parameters. The reservation wage takes the value around 2.3.\nThe simplest way to obtain it is to observe that the policy function (indexed by 1 and 2 here) jumps from not accepting to accepting at the reservation wage.\nThis can be easily implemented with the findfirst() built-in function. Of course, you can alternatively write a loop over pol and use an if to find the index and break after the condition is met.\n\nfunction Reserve_w(w_parameter)\n    w = w_parameter.w\n    (v, βEv, pol, iterate) = Mccall_discrete(w_parameter)\n    \n    idx = findfirst(==(2), pol)\n    \n    return w[idx]\nend\n\nw1 = Reserve_w(w_parameter)\n\n2.3434343434343434\n\n\n\n\n3.4 Comparative Statics\nSince the employee’s decision is solely characterized by the reservation wage, looking at the impact of changing some of the model’s parameters on the reservation wage gives us a lot of insights.\nFirst, let’s think of the impact of a change in \\(\\beta\\) will have on the reservation wage. I make \\(\\beta\\) go up from 0.7 to 0.9.\n\nw2 = Reserve_w(worker(β = 0.9)) # easy change of parameter with struct\n\n2.909090909090909\n\n\n\nw2 &gt; w1\n\ntrue\n\n\nOther things being equal, the reservation wage is higher. That is, the worker is willing to wait longer for a higher paying job. This is consistent with the typical interpretation of higher \\(\\beta\\) as being more patient.\nNext up, let’s look at a change in the unemployment benefit \\(b\\). Let’s decrease the reservation wage \\(b\\) from \\(1.5\\) to \\(1.0\\).\n\nw3 = Reserve_w(worker(b = 1.0)) # easy change of parameter with struct\n\n2.101010101010101\n\n\n\nw3 &lt; w1\n\ntrue\n\n\nA decrease in unemployment benefit results in a smaller reservation wage (and lowers worker’s welfare). Intuitively, the opportunity cost of not accepting an offer is higher. An worker is then willing to take offers that pays less. This result can be formally proven. See Tim’s Notes.\nAt last, let’s look at the effect of a mean-preserving spread of the wage distribution on the reservation wage. A mean-preserving spread is a special case of second-order stochastic dominance – namely, the special case of equal means. Let A and B be two distributions. If B is a mean-preserving spread of A, then A is second-order stochastically dominant over B; and the converse holds if A and B have equal means.\nA necessary (not sufficient) condition for mean-preserving spread is higher variance.\nHere we can easily construct a mean-preserving spread for \\(P\\) with putting equal weights only on the first and last index of \\(w\\). (with 0.5 prob., worker gets 0 and with 0.5 prob., worker gets 4)\n\nN = w_parameter.N\nP_spread = zeros(N)\nP_spread[1] = 0.5\nP_spread[N] = 0.5\nw4 = Reserve_w(worker(P = P_spread))\n\n2.8686868686868685\n\n\n\nw4 &gt; w1\n\ntrue\n\n\nIt might seem surprising at first glance that a mean preserving spread actually increases worker’s reservation wage (in this case also worker’s welfare). This is because by the setup of the problem, the worker only cares about the right-tail of the distribution - she can always decline an offer that she doesn’t like. Intuitively, the worker will prefer a wage distribution that has equal mean but higher variance. The theoretical derivation for this result can again be seen in Tim’s Notes.\n\n\n3.5 Stopping time\nWe can compute the mean stopping time of job search. This is the expected periods of an unemployed worker finding a job.\nHere since we assumed an uniform distribution. We know the probability of an unemployed worker accepting the offer each period is \\(\\frac{\\bar{w}}{4.0}\\), where \\(\\bar{w}\\) is the reservation wage. We can compute the stopping time numerically:\n\nfunction stopping_time(w_parameter, reserv_wage; repeat = 1000)\n    stop_time = zeros(Int, repeat)\n    (;w, P) = w_parameter\n    for i in 1:repeat\n        t = 0 #initialization   \n        \n        while true\n            wage = draw_wage(w, P)\n\n            if wage &lt; reserv_wage\n                t = t + 1\n            else \n                stop_time[i] = t\n                break\n            end\n        end\n    end\n    \n    return mean(stop_time)\nend\n\nfunction draw_wage(w, P)\n    # typical way of how to draw a random number from arbirtrary distribution\n    x = rand() #random number from 0 to 1\n    temp = 0.0\n    wage = zero(eltype(w))\n    for i in eachindex(P)\n        if x &gt; temp\n            temp = temp + P[i]\n        else\n            wage = w[i]\n            break\n        end\n    end\n    return wage\nend\n\ndraw_wage (generic function with 1 method)\n\n\n\nstopping_time(w_parameter, w1)\n\n1.461\n\n\n\nstopping_time(w_parameter, w2)\n\n2.654\n\n\nHolding the distribution constant, higher reservation wage implies longer waits.\n\n\n3.6 continuous distribution (follows from Quantecon)\nNow let’s add some continuous distributional assumptions for the wage offer. Check out how to work with the “Distributions” package in the first problem set.\nWith no particular reason, let the wage offer follow a Log-normal distribution with unit shape and unit scale.\nLet’s take a look at how the distribution looks like.\n\nwdist_con = LogNormal() \n\nplot(0:0.1:10, pdf.(wdist_con, 0:0.1:10), xlabel = \"wages\", ylabel = \"probabilities\", legend = false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwdist_con\n\nLogNormal{Float64}(μ=0.0, σ=1.0)\n\n\n\n\nChallenge: need to compute the numerical expectation\nTo solve the household problem, we need to solve for the constant \\(b + \\beta \\int_0^B V(w')dF(w')\\). However, we can’t analytically solve for \\(b + \\beta \\int_0^B V(w')dF(w')\\) as we only have a numerical approximation of \\(V(.)\\). Moreover, the computer can’t deal with continuous grids. In particular, \\(V(.)\\) has to be defined on discrete grids, so we have to numerically approximate the integral \\(\\int_0^B V(w')dF(w')\\).\nWe can use the Expectations package, which automatically finds the best algorithm to compute numerical expectations (usually Gaussian Quadrature).\n\n# struct of parameters\nBase.@kwdef struct worker_continuous \n    β :: Float64 = 0.7\n    b :: Float64 = 2 # unemployment benefit\n    N :: Int64 = 100\n\n    wdist_continuous :: Sampleable = LogNormal(0.0, 1.0) # root type for distributions\n    \nend\n\nw_continuous = worker_continuous()\n\nworker_continuous(0.7, 2.0, 100, LogNormal{Float64}(μ=0.0, σ=1.0))\n\n\n\nLogNormal(0.0, 1.0)\n\nLogNormal{Float64}(μ=0.0, σ=1.0)\n\n\n\nfunction Mccall_continuous_package(w_continuous; N = 100, tol = 1e-6 ,  v = zeros(N))\n     # unpacking \n    (; β, b, wdist_continuous) = w_continuous\n    \n    v1 = similar(v)\n    E = expectation(wdist_continuous; n = w_continuous.N ) # expectation operator\n    \n\n    iterate = 0\n\n    while true  \n        distance = zero(eltype(v))\n        \n        v1 = max.(w/(1 - β), b + β * E * v)\n\n        distance = maximum(abs.(v1 - v))\n        \n        (distance &lt; tol || iterate == 1000) && break # break out of the whole loop if one of the statements is true\n        iterate += 1\n\n        # Vectorized update of v using element-wise assignment\n        v .= v1\n    end\n    return (v = v, w=w, iterate = iterate)\nend\n\n\nMccall_continuous_package (generic function with 1 method)\n\n\n\nv, w, = Mccall_continuous_package(w_continuous)\n\n(v = [6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644, 6.870217689680644  …  12.12121212121212, 12.255892255892254, 12.39057239057239, 12.525252525252524, 12.659932659932657, 12.794612794612792, 12.929292929292927, 13.063973063973062, 13.198653198653197, 13.333333333333332], w = 0.0:0.04040404040404041:4.0, iterate = 18)\n\n\n\nv1 = 1/(1 - w_continuous.β) .* w\nv2 = similar(w)\nplot(w, v1, line = (:red, 1), label = L\"\\frac{w}{1 - β}\")\nplot!(w, ones(w_continuous.N).*v[1], line = (:green, 1), label = L\"\\bar{V}\")\nplot!(w, v .+ 0.1, line = (:blue, 2.5), label = L\"V(w)\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe obtain the expected result.\n\n\n3.7 Conclusion\nIn this lecture, we computed the most basic Mccall job search model. We also discussed some of the model’s properties with numerical results. Further readings for this topic on Quantecon Lecture 28 - 33 are highly recommended."
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_2_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_2_Compecon_Xing_Xu.html",
    "title": "Lecture 2: A canonical Neo-classical growth model and introduction to value function iteration (VFI)",
    "section": "",
    "text": "Xing Xu, University of Minnesota, 2023 Summer"
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_2_Compecon_Xing_Xu.html#conclusion",
    "href": "Julia/Lectures/Julia_Lec_2_Compecon_Xing_Xu.html#conclusion",
    "title": "Lecture 2: A canonical Neo-classical growth model and introduction to value function iteration (VFI)",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nWe have gone carefully through solving a very standard problem in Macroeconomics. The goal is to showcase how to perform basic value function iteration in Julia. We mainly talked through how to code the problem in loop version and vector version. Along the way, I showed some important features of Julia coding. Most importantly, loops, functions, struct, broadcasting and vector/matrix operations.\nAt last, we compared the performance of coding with loops and vectorizations. It might occur to you that loop version has better performance. Please keep in mind this varies in different problems and it is useful to know both methodologies."
  },
  {
    "objectID": "Julia/Assignments/Julia_PS_2_Compecon_Xing_Xu.html",
    "href": "Julia/Assignments/Julia_PS_2_Compecon_Xing_Xu.html",
    "title": "Question 2.1: Adding labor supply to the model.",
    "section": "",
    "text": "One natural extension to the problem is to add preferences for leisure to characterize labor supply changes.\nDenote the labor supply as \\(l_t\\). For simplicity, let household has 1 unit of labor endowment each period. Leisure can thus be denoted as \\(1 - l_t\\). Now, consider the following problem:\n\\[\\begin{align*}\n    V(k_0) \\equiv \\max_{\\{c_t, l_t, k_{t+1}\\}_{t=0}^\\infty} &\\sum_{t=0}^\\infty \\beta^t u(c_t, 1 - l_t) \\\\\n    \\text{subject to } c_t + k_{t+1} &\\leq F(k_t, l_t) + (1 - \\delta) k_t, \\forall t\\\\\n    c_t, k_{t+1} \\geq 0,& l_t \\in [0,1], \\forall t \\\\\n       \\quad k_0 &\\text{ given }\n\\end{align*}\\]\nLet the utility be denoted as:\nExercise: Find the Euler equation as before. What changed? In addition, use the FOCs from \\(c_t\\) and \\(l_t\\) to characterize the intratemporal substitution between consumption and leisure. Make some economic justifications for the results.\nThe recursive formulation of the problem is:\n\\[\\begin{align*}\n    V(k) = & \\max_{c,l, k'} \\{u(c, 1 - l) + \\beta V(k') \\}\\\\\n    \\text{subject to } & c + k' \\leq F(k, l) + (1 - \\delta) k\\\\\n    & c , k' \\geq 0, l \\in [0, 1]\n\\end{align*}\\]\nAgain, we can substitute in \\(c\\) and write the problem as the following,\n\\[\nV(k) =  \\max_{l, k' \\in \\Gamma(k)} \\{u(F(k, l) + (1 - \\delta) k - k', 1 - l) + \\beta V(k')\\}\n\\] where \\(\\Gamma(k) \\equiv \\{k', l | l \\in [0,1], k' \\in [0, F(k, l) + (1 - \\delta) k]\\}\\)\n\nSolving the problem\nLet’s specify some parameter values as before. Again we take use of struct.\n\nBase.@kwdef struct Household_labor # or simply @kwdef\n    σ :: Float64 = 2.0\n    β :: Float64 = 0.9\n    A :: Float64 = 2.0\n    α :: Float64 = 0.7\n    δ :: Float64 = 0.9\n    γ :: Float64 = 0.3 # additional parameter for the utility weight between consumption and leisure\nend\n\nh_l = Household() \n\nHousehold(2.0, 0.9, 2.0, 0.7, 0.9)\n\n\nIn addition, we need to specify the utility function and the production function. We use a separable CRRA utility for consumption and leisure. Production function is the same.\n\nu2(c,l, m) = γ * c^(1.0 - m.σ)/(1.0 - m.σ) + (1.0 - γ) * (1.0 - l)^(1.0 - m.σ)/(1.0 - m.σ)\nF2(k,l, m) = m.A * k^m.α * l^(1.0 - m.α)\n\nF2 (generic function with 1 method)\n\n\nLet’s build towards solving the model. First, note that the maximum sustainable capital is the same as before. Again, \\(F(\\tilde{k}, 1) = \\delta \\tilde{k}\\), which is \\(A \\tilde{k}^\\alpha = \\delta \\tilde{k}\\). Since \\(\\delta &gt; 0\\), \\(\\tilde{k} = (\\frac{A}{\\delta})^{\\frac{1}{1 - \\alpha}}\\). Intuitively, this is because that if we let consumption be \\(0\\) and use all labor endowment for production. The maximum sustainable level for capital in the long term will be when \\(F(\\tilde{k}, 1) = \\delta \\tilde{k}\\).\nEverything else basically follows, except that we also need to find optimal labor supply (or leisure). There are two ways to do this. You will write the codes for VFI with the algorithms provided below. Try to write all codes by yourself and check with the basic model without labor supply decisions in the lecture.\nThe brute force way of solving this starts with constructing an additional grid for labor and search over maximized value with different combinations of \\(k'\\) and \\(l\\). Since \\(l \\in [0,1]\\), we can initialize \\(l\\) with range(0.0, 1.0, 10) (10 grids for simplicity).\nThe algorithm is very similar to that in the lecture. We need an additional loop over possible values of \\(l\\). You will also need two policy functions, pol_kprime for storing optimal policy index for \\(k'\\) and \\(pol_l\\) for \\(l\\). For each combination of \\(k'\\) and \\(l\\), compute \\(c\\) and check whether it is non-negative.\nExercise: Write the function for performing VFI for the new model. Write the loop version first. After that, plot the value function and the policy function for \\(k'\\) and \\(l\\) given each \\(k\\).\nThink: does the trick with strictly increasing policy function we used in the lecture still apply here? If so how can we implement it?\nExercise: Vectorize your code as in the lecture. (partially first and try full vectorization if you are confident) Check with the result from the loop version."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xing Xu",
    "section": "",
    "text": "I am a fourth year PhD candidate in Economics at the University of Minnesota and a research analyst at the Federal Reserve Bank of Minneapolis. I work broadly on the topics under Macroeconomics and International Economics.\n\nCV Research Teaching GitHub Email\n\n\n\n\n\n\nportrait"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Xing Xu",
    "section": "News",
    "text": "News\n\n2025-08 — New draft available on NBER: Openness and Growth: A Comparison of the Experiences of China and Mexico (joint with Tim Kehoe)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Xing Xu",
    "section": "Contact",
    "text": "Contact\nDepartment of Economics, University of Minnesota\nEmail: xu000956@umn.edu\nCV, papers, and teaching materials are updated regularly."
  },
  {
    "objectID": "Julia/Assignments/Julia_PS_1_Compecon_Xing_Xu.html",
    "href": "Julia/Assignments/Julia_PS_1_Compecon_Xing_Xu.html",
    "title": "Problem set 1",
    "section": "",
    "text": "Xing Xu, University of Minnesota, 2023 Summer"
  },
  {
    "objectID": "Julia/Assignments/Julia_PS_1_Compecon_Xing_Xu.html#question-2",
    "href": "Julia/Assignments/Julia_PS_1_Compecon_Xing_Xu.html#question-2",
    "title": "Problem set 1",
    "section": "Question 2",
    "text": "Question 2\nExercise: Simple Monte-Carlo with Linear regression (taken from Amil Petrin’s Econometric class Homework)\nThis requires some basic knowledge on OLS.\nThe objective of a Monte Carlo simulation is to verify finite and large sample results. Given a model for how variables are determined, one can test the performance and properties of sample estimators by generating several samples drawn from the same population. We will operate with the following model: \\[\ny_i=\\beta_0+x_i \\beta_1+\\epsilon_i\n\\] where \\(y_i\\) and \\(x_i\\) are random variables. You observe \\(N\\) samples of \\(i=1, \\ldots, n\\) observations of \\(y\\) and \\(x\\). You will estimate: \\[\ny_i=\\hat{\\beta}_0+x_i \\hat{\\beta}_1+e_i\n\\] You will have to execute a variety of simulations, changing sample sizes, number of samples, and assumptions. 1. First assume that the \\(\\epsilon\\) and the regressors are uncorrelated. Assume that both \\(X\\) and \\(\\epsilon\\) are drawn from a normal distribution \\(\\epsilon \\sim N\\left(0, \\sigma_\\epsilon^2\\right)\\), \\(X \\sim N\\left(\\mu_x, \\sigma_x^2\\right)\\). Start with a sample size \\((n)\\) of \\(n=100\\) and define the number of samples to be simulated \\((N)\\) to be \\(N=100\\) as well. You also need to define the true values of \\(\\beta_0\\) and \\(\\beta_1\\). Report the results that you find for the means and variance-covariance matrix of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\). Finally, plot the distribution of the estimated parameters and the theoretical normal distribution that they are supposed to follow. 2. Now vary the sample size \\(n\\) and redo 1) under the new values. Report and plot your results. Comment. 3. Now vary the number of samples \\(N\\) and redo 1) under the new values. Report and plot your results. Comment. 4. Now vary the distribution of the \\(X\\) and redo 1) under the new assumptions. Report and plot your results. Comment. 5. Now set the mean of the error \\(\\epsilon\\) to be different from zero and redo 1) under the new values. Comment. 6. Now draw the \\(\\epsilon\\) in such a way that it is in fact correlated with \\(X\\) and redo 1) under the new assumptions. Report and plot your results. Comment."
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_5_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_5_Compecon_Xing_Xu.html",
    "title": "Firm dynamics with an example of exporter dynamics",
    "section": "",
    "text": "1 What is firm dynamics?\nThe firm dynamics literature wants to study firms’ behavior over time. Dynamic models were developed to study firms’ entry and exit decisions, age, size distribution over time and even R& D, export and FDI decisions, etc. More recently, spatial components are incorporated to study, say, firms’ heterogeneous location choices for their subsidiaries.\nTo dig into the literature, a good starting point is Hopenhayn (1992), which provides a general theoretical framework that features uncertainty towards future profits, costly market entry and endogenous exit. The literature then expands dramatically with various focuses. Just to list a few, Klette and Kortum on innovations and firm distribution, Cooley and Quadrini (2001) on financial frictions, Das et al. (2007) on exporter dynamics.\n\n\n2. What’s specific about exporter dynamics?\nIn a world where large multinationals take up a huge chunk of global economy via exports, the roles these giants play is a very interesting research topic. The OECD estimated in 2018 that multinational corporations account for half of global exports, nearly a third of world GDP (28%), and about a fourth of global employment. They hold great market power and sometimes even directly alter domestic and foreign policies. Understanding their growth and policy implications is a fascinating ongoing topic, which requires a lot of the tools discussed here. We can use the dynamic trade framework to study firms’ individual and aggregate behavior, international policies and their implications on consumer welfare.\nFor the remaining part of this section, I will discuss some empirical evidence about firm-level exporting behavior. I mainly follow from the literature review on firm dynamics and trade by George, Costas and Kim, ARE 2021.\n\n\n3. Firm-level facts (from firm-level data of US and Columbia):\nStatic or time-invariant facts: * There are few exporters. * Exports are a small share of an exporter’s total sales. * Exporters are relatively large firms.\nThese facts can already trigger our thinking about the underlying firm distribution in size and productivity. These are consistent with the results of Melitz (2003). Within a group of firms originally producing domestically, only the most productive firms enter the export market. They stay in the market and force the less productive firms to exit.\nIf we further take a look at the dynamic aspect of the data, there are some more interesting observations:\n\nPast export participation is the main predictor of current export participation.\nExporter are less likely to exit if they are older and larger exporters in the past.\nEntry rate is increasing in size and past export activity.\nThe longer an exporter stays in the market, the more it tends to export.\n\nThese can explain why the dynamic aspect is vital to studying the exporting firms’ behavior. I will show some predictions of the results when we go to the theoretical framework.\nThere are also evidences about heterogeneity between exporters on their export destinations, shipment volumes and frequencies and inventories.\nAt last, The long-run response of aggregate trade volumes to changes in trade policy is larger than the short-run response. An important reason, as shown in Kehoe and Ruhl (2013), is that the aggregate trade growth is stronger in the long term due to the extensive margin. That is, during a trade liberalization, previously non-traded products grow faster than products that have already been traded between the same country pairs. One take-away is that larger quantities of firms (for concreteness, plants) would become exporters when there are ease in bilateral trade policies. Combining with the fact that exporters take time to grow, this gives a possible explanation for the long-run growth of trade.\n\n\n4. Some questions to think about:\nOn building a model, what structures do we need on individual firm’s problem to retrieve the same results as we observe? For example, how to make sure that only a few firms enter into the export market and coincidentally they are relatively large? What about on the market structure and the demand side?\nNext, we can think about idiosyncratic and aggregate uncertainties. Idiosyncratic shock is convenient to generate endogenous entry and exit, as in the firm dynamics literature. Now we can look at if there are any heterogeneous short and long run responses to aggregate uncertainties, say real exchange rate fluctuations. If so, what are the policy implications?\nA very important topic in the trade literature is the gains from trade. That is the welfare gain of household in a country facing some sort of trade liberalization. Although the focus of our model is mainly on the firm side, we can still analyze the aggregate impact under a general equilibrium framework.\n\n\n5. A canonical model for exporter dynamics\nThe model is a slightly tweaked version of Das et al. (2007), based upon Melitz (2003). For further reference, Ruhl and Willis (2017) and Alessandria et al. (2021).\nThis section is arranged as follows. I will first lay out the static model and provide analytical solutions to it. Next, I will provide the dynamic setup of the model and compute the model. To look at the firms’ short-term reaction to some sudden and unexpected (MIT) shocks, I will then illustrate the algorithms for obtaining the transition dynamics. In the end, I will provide some basic calibration and some preliminary results. I run some experiments and look at the exporters’ full dynamics facing different shocks.\n\n5.1 Static model\n\n5.1.1 Consumer\nA representative consumer in domestic economy has preferences over an aggregate consumption good, \\(C\\). Each index \\(j\\) denotes a differentiated variety. \\(C\\) takes the form: \\[\n    C = \\left(\\sum_{j=1}^{J} c_j^\\frac{\\theta - 1}{\\theta} \\right)^\\frac{\\theta}{\\theta - 1},\n\\] where \\(\\theta\\) is the elasticity of substitution between varieties, and J is the number of available varieties. The consumer maximize his utility subject to the budget constraint: \\[\n    \\sum_{j=1}^{J} c_j p_j = I.\n\\] The consumer takes prices as given and chooses optimal consumption \\(c_j\\) of each variety: \\[\n    c_j = (\\frac{p_j}{P})^{-\\theta}C,\n\\] where P is an aggregate price level given by: \\[\n    P = \\left(\\sum_{j=1}^{J} p_j^{1-\\theta} \\right)^{\\frac{1}{1 - \\theta}}.\n\\] The foreign market is identical to the domestic market with demand and consumption satisfying: \\[\n    \\begin{aligned}\n    c_j^* &= (\\frac{p_j^*}{P^*})^{-\\theta}C^*, \\\\\n    P^* &= \\left(\\sum_{j=1}^{J} {p_j^*}^{1-\\theta} \\right)^{\\frac{1}{1 - \\theta}}. \\\\\n    \\end{aligned}\n\\] Countries have same elasticity \\(\\theta\\) for differentiated goods but differ in aggregate level of demands and prices.\n\n\n5.1.2 Firm’s static problem\nAssume each firm operates solely on a market \\(j\\), so “plants” will be a better term than “firms”. Plant operates monopolistically in each differentiated variety. Plant chooses the optimal amounts to produce in domestic and foreign market. A unit good produced with plant-level technology $_j $, labor \\(n_j\\) and capital \\(k_j\\) subject to: \\[\n    f(\\tilde{\\epsilon}_j, n_j, k_j) = \\tilde{\\epsilon}_j  n_j^{\\alpha_N}k_j^{\\alpha_K},\n\\] where we assume nonincreasing returns to scale of the production technology, \\(\\alpha_N + \\alpha_K &lt;= 1\\).\nIn each period, the plant chooses the prices, production, inputs, and export status next period \\(X'\\) (\\(X'=1\\) if exporting next period and \\(X'=0\\) if not) to maximize its infinite horizon utility. The plant also faces an iceberg cost \\(\\xi_j\\), where fraction \\(\\xi_j - 1\\) of an export shipment is destroyed in transportation. In addition, foreign market might put an ad valorem tariff \\(\\tau_j\\) on exports.\nWe can thus first solve for the plant’s single period or static problem given all the states and plant’s export decision last period. A plant’s profit is sum of revenue from domestic and foreign sales less the input costs: \\[\n\\Pi_j =  p_j y_j + I(X_j = 1) (1 - \\tau_j) Q p_j^* y_j^* - wn_j - rk_j,\n\\] where \\(Q\\) is the real exchange rate; r is the rental rate of capital; and w is the wage. The plant is also subject to its feasibility constraint: \\[\ny_j + \\xi_j y_j^* = \\tilde{\\epsilon}_j  n_j^{\\alpha_N}k_j^{\\alpha_K}.\n\\] With market clearing condition in both markets \\(c_j = y_j\\) and \\(c^*_j = y^*_j\\) if \\(X_j = 1\\), with demand functions from the household problem. The plant charges constant markup over marginal cost in both markets: \\[\n    \\begin{aligned}\n    p_j &= \\frac{\\theta}{\\theta - 1} {MC}_j \\\\\n    Q p_j^* &= \\frac{1}{1- \\tau_j} \\frac{\\theta}{\\theta - 1} \\xi_j {MC}_j.\n    \\end{aligned}\n\\] where \\(MC_j = \\frac{1}{\\tilde{\\epsilon_j}} \\left(\\frac{w}{\\alpha_n}\\right)^{\\alpha_n} \\left(\\frac{r}{1 - \\alpha_n}\\right)^{1 - \\alpha_n}\\) with \\(\\alpha_n + \\alpha_k = 1\\) (if not, the formula for marginal cost will be more complicated).\nThe plant’s static profit maximization problem is: \\[\n\\Pi_j = \\max_{y_j, y_j^*}\\{ P C^{\\frac{1}{\\theta}} y_j^\\frac{\\theta - 1}{\\theta} +  I(X_j = 1) (1 - \\tau_j) Q P^* {C^*}^{\\frac{1}{\\theta}}  {y_j^*}^\\frac{\\theta - 1}{\\theta} - wn_j - rk_j\\},\n\\] subject to equation the feasibility constraint. The optimal amounts for a plant to produce in domestic and foreign markets are: \\[\n\\begin{aligned}\ny_{j} &=\\frac{ \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta}   (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}}{I \\left(X_{j}=1\\right) \\xi_j + \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta} (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}} \\tilde{\\epsilon}_{j} n_{j}^{\\alpha_{N}} k_{j}^{\\alpha_{K}} \\\\\ny_{j}^{*} &= \\frac{I(X_j = 1)}{\\xi_j+ \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta} (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}} \\tilde{\\epsilon}_{j} n_{j}^{\\alpha_{N}} k_{j}^{\\alpha_{K}} .\n\\end{aligned}\n\\]\nSubstituting in \\(y_j\\) and \\(y_j^*\\) to the profit maximization problem yields: \\[\n\\begin{aligned}\n\\Pi\\left(X_{j}, \\xi_j,  \\epsilon_{j}, Q\\right)=&\\max _{n_{j}, k_{j}} M n_{j}^{\\alpha_{N} \\frac{(\\theta-1)}{\\theta}} k_{j}^{\\alpha_{K} \\frac{(\\theta-1)}{\\theta}} - wn_{j}- r k_{j},\n\\end{aligned}\n\\] where \\(M = \\left(1+I\\left(X_{j}=1\\right) \\xi_j^{1 - \\theta} (1 - \\tau_j)^\\theta \\left(\\frac{Q P^*}{P}\\right)^{\\theta} \\frac{C^{*}}{C}\\right)^{\\frac{1}{\\theta}} P C^{\\frac{1}{\\theta}} \\tilde{\\epsilon}_{j}^{\\frac{\\theta-1}{\\theta}}\\)\nFrom first order conditions of labor and capital, for simplicity write \\(a = \\alpha_{N} \\frac{(\\theta-1)}{\\theta}\\) and \\(b = \\alpha_{K} \\frac{(\\theta-1)}{\\theta}\\), the maximized profit can be solved as: \\[\n\\Pi\\left(X_{j}, \\xi_j, \\epsilon_{j}, Q\\right) = M^{\\frac{1}{1-a-b}} (1-a-b) (\\frac{a}{w})^{\\frac{a}{1-a-b}} (\\frac{b}{r})^{\\frac{b}{1-a-b}}.\n\\] Equate \\(\\xi_j = 1\\) you get exactly the plant’s profit without export iceberg cost. Then with only export fixed cost driving the export barrier, the model is equivalent to the baseline sunk cost model from . Assuming no tariff, using the same notation, the profit a plant earns with an iceberg cost is exactly \\(\\left({\\frac{1+I\\left(X_{j}=1 \\right) \\xi_j^{1 - \\theta} \\left(\\frac{Q P^*}{P}\\right)^{\\theta} \\frac{C^{*}}{C}}{1+I\\left(X_{j}=1\\right) \\left(\\frac{Q P^*}{P}\\right)^{\\theta} \\frac{C^{*}}{C}}}\\right)^\\frac{1}{\\theta (1- a-b)}\\) of the same plant without an iceberg cost.\nJust for clarification, we can also solve for domestic and foreign sales: \\[\n    \\begin{aligned}\n       p_j y_{j} &= \\left(\\frac{ \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta} (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}}{I \\left(X_{j}=1\\right) \\xi_j + \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta} (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}}\\right)^{\\frac{\\theta - 1}{\\theta}}   \\\\\n       & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P C^{\\frac{1}{\\theta}} \\epsilon_j M^{\\frac{a + b}{1-a-b}} (\\frac{a}{w})^{\\frac{a}{1-a-b}} (\\frac{b}{r})^{\\frac{b}{1-a-b}} \\\\\n       Q p_j^* y_j^* &= I(X_j = 1) \\left(\\frac{1}{\\xi_j+ \\xi_j^{\\theta} (1 - \\tau_j)^{-\\theta} (\\frac{P}{Q P^*})^\\theta \\frac{C}{C^{*}}}\\right)^{\\frac{\\theta - 1}{\\theta}} \\\\\n       & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  Q P^* {C^*}^{\\frac{1}{\\theta}} \\epsilon_j M^{\\frac{a + b}{1-a-b}} (\\frac{a}{w})^{\\frac{a}{1-a-b}} (\\frac{b}{r})^{\\frac{b}{1-a-b}} .\n    \\end{aligned}\n\\]\nWe normalize the size of domestic aggregate demand \\(C\\) to 1, so \\(C^*\\) denotes the size of foreign demand relative the domestic demand. Define \\(\\epsilon_j = {\\tilde{\\epsilon}_j}^\\frac{\\theta - 1}{\\theta}\\), which follows an idiosyncratic AR1 shock for each plant. We normalize the mean of the \\(\\epsilon\\) process to one.\nFirms face idiosyncratic productivity shocks, a fixed cost for becoming an exporter abd and an iceberg cost for exports.\n\n\n\n5.2 Firm’s Dynamic problem\nWe first introduce an export fixed cost structure as in Ruhl and Willis (2017). The plant faces a sunk entry cost \\(f_E\\) and a continuation cost \\(f_C\\), \\(f_C \\leq f_E\\). All export decisions are made at the end of previous period and fixed costs are paid upon decision. The exporting cost of each period can be expressed as: \\[\n    f(X, X') = (1-X) * f_E * X' + X * f_C * X' .\n\\]\nWe then link the export decision of a plant with an endogenous and stochastic iceberg transportation cost \\(\\xi_j\\) as in Alessandria et al. (2021). A non-exporting plant doesn’t ship any goods abroad and we set its iceberg cost this period to infinity. The nonexporter can deterministically lower its fixed cost next period to \\(\\xi_E\\) by paying \\(f_E\\). If a current exporter pays \\(f_C\\) to continue exporting next period, it draws iceberg cost \\(\\xi_j \\in \\{\\xi_E, \\xi_C\\}\\) next period stochastically. The structure of the iceberg cost conditional on continuing to exporting can be summarized by a markov chain with states \\(\\xi_E\\) and \\(\\xi_C\\) with transition probability \\([ \\rho_{EE}, 1- \\rho_{EE} ; 1 - \\rho_{CC}, \\rho_{CC} ]\\). Assume \\(\\xi_C \\leq \\xi_E &lt; \\infty\\). If an exporter stops paying \\(f_C\\), it exits the export market and becomes a nonexporter with infinite iceberg cost next period.\nWe also have a persistent condition as \\(\\rho_\\xi(\\xi_E|\\xi_C) \\leq \\rho_\\xi(\\xi_C|\\xi_E)\\). With a lot of plants, this particular structure of iceberg cost creates incentives for some plants to invest in export technology and make the “better” exporters to become less susceptible to unfavorable shocks.\nOne last component is the technological shock \\(\\epsilon_j\\) that governs plant productivity, which generates heterogeneous plants and varying entry and exit export decisions of plants. It follows an idiosyncratic and time-invariant AR(1) processes for each plant: \\[\\begin{equation}\n\\ln{\\epsilon_t} = \\rho_\\epsilon\\ln{\\epsilon_{t-1}} + w_{\\epsilon,t}, \\hspace{1cm} w_\\epsilon \\sim N(0, \\sigma^2_\\epsilon).\n\\end{equation}\\] The shocks are discretized using Tauchen’s method. There are three key parameters to determine here: {{\\(N_\\epsilon\\), \\(\\rho_\\epsilon\\), \\(\\sigma_\\epsilon\\)}}. \\(N_\\epsilon\\) is the states of plant’s productivity, currently set to 1000 for simplicity.\nWe put aside our discussion of exchange rate shocks for the moment as an aggregate shock will make solving for the equilibrium prices and transition dynamics much harder. Exchange rates directly shift the export prices and create more uncertainties which make the foreign market more volatile. In particularly “bad” periods, there can be no firms deciding to export, leading to potentially poor simulation results.\nIn each period, every plant makes an discrete choice based on their current status and discounted expectation of future profits. The discount rate \\(R\\) is set to be \\(\\frac{1}{1+r}\\). We characterize the dynamic problem of each plant by the following Bellman equation: \\[\nV\\left(X_{j}, \\xi_j, \\epsilon_{j}, Q\\right)=\\max _{X_{j}^{\\prime}}\\left\\{\\Pi\\left(X_{j}, \\xi_j, \\epsilon_{j}, Q\\right)-f\\left(X_{j}, X'_{j} \\right)+R \\underset{\\xi_j^{\\prime}, \\epsilon_{j}^{\\prime}, Q^{\\prime}}{\\mathbb{E}} V\\left(X_{j}^{\\prime}, \\xi_j^{\\prime}, \\epsilon_{j}^{\\prime}, Q^{\\prime}\\right)\\right\\}.\n\\]\nThe policy function for export entry is thus: \\[\n    X_{j}^{\\prime}\\left(0, \\infty, \\epsilon_{j}, Q\\right)= \\begin{cases}1 & \\text{ if } \\hspace{0.5cm}  - f_E + \\\\\n{} & R \\underset{\\xi_j^{\\prime}, \\epsilon_{j}^{\\prime}, Q^{\\prime}}{\\mathbb{E}} \\left[V\\left(1, \\xi_E, \\epsilon_{j}^{\\prime}, Q^{\\prime}\\right)-V\\left(0, \\infty, \\epsilon_{j}^{\\prime}, Q^{\\prime}\\right)\\right] \\geq 0 \\\\ 0 & \\text { otherwise. }\\end{cases}\n\\]\nThe inequality on the right is the condition of a non-exporting plant choosing to be an exporter next period. The first two terms denote the current period profit minus the entry fixed cost. The discounted expectation is the difference of expected future values between starting to become an exporter with a low export technology \\(\\xi_E\\) and staying as an nonexporter.\n\n\n\n6. Solving the model\nAs always, let’s input some parameter value. I made the simplifying assumption that the real exchange rate \\(Q\\) is fixed.\n\nusing LinearAlgebra, Statistics, Distributions\nusing Plots\n\n\nBase.@kwdef struct Exporter{T1, T2, R1, S}\n    r::R1 = 0.109\n    αn::R1 = 0.45\n\n    αk::R1 = 1.0 - αn\n    θ::R1 = 5.0\n    w::R1 = 0.02\n\n    ξE::R1 = 1.6\n    ξC::R1 = 1.2\n    ξ::T1 = [ξE; ξC]\n    \n    ρEE::R1 = 0.92\n    ρCC::R1 = 0.92\n    Ξ::T2 = [ρEE (1.0-ρEE); (1.0-ρCC) ρCC]\n\n    Q::R1 = 1.0 # fix exchange rate for the moment\n\n    fE::R1 = 0.7\n    fC::R1 = 0.35\n    \n    ρϵ::R1 = 0.872524\n    σϵ::R1 = 0.115886\n\n    Cstar::R1 = 0.7 #C normalized to be 1.0\n\n    grids::S = 100\n\n    R = (1/(1+r))^0.25 # quarterly discount factor\nend\n\n\nE = Exporter() \n\nExporter{Vector{Float64}, Matrix{Float64}, Float64, Int64}(0.109, 0.45, 0.55, 5.0, 0.02, 1.6, 1.2, [1.6, 1.2], 0.92, 0.92, [0.92 0.07999999999999996; 0.07999999999999996 0.92], 1.0, 0.7, 0.35, 0.872524, 0.115886, 0.7, 100, 0.9744669483879411)\n\n\n\n6.1 Discretize an AR(1) process\nRecall that the reason we use discrete approximations of value functions is that the computer can’t process continuos variables. Same logic applies with an AR(1) process. We want to approximate the AR(1) process with a discrete markov chain, which is characterized by discrete states and a transition probability matrix. Check Quantecon’s lecture notes for a detailed explanation.\nAgain the AR(1) process we want to discretize is: \\[\n\\ln{\\epsilon_t} = \\rho_\\epsilon\\ln{\\epsilon_{t-1}} + w_{\\epsilon,t}, \\hspace{1cm} w_\\epsilon \\sim N(0, \\sigma^2_\\epsilon).\n\\]\nThe code (adopted from Quantecon) is:\n\nfunction tauchen(N::Integer, ρ::T1, σ::T2, μ=zero(promote_type(T1, T2)), n_std::T3=3) where {T1 &lt;: Real, T2 &lt;: Real, T3 &lt;: Real}\n  # Get discretized space\n  a_bar = n_std * sqrt(σ^2 / (1 - ρ^2))\n  y = range(-a_bar, stop=a_bar, length=N)\n  d = y[2] - y[1]\n\n  # Get transition probabilities\n  Π = zeros(promote_type(T1, T2), N, N)\n  for row = 1:N\n      # Do end points first\n      Π[row, 1] = cdf(Normal(),(y[1] - ρ*y[row] + d/2) / σ)\n      Π[row, N] = 1.0 - cdf(Normal(),(y[N] - ρ*y[row] - d/2) / σ)\n\n      # fill in the middle columns\n      for col = 2:N-1\n          Π[row, col] = (cdf(Normal(),(y[col] - ρ*y[row] + d/2) / σ) -\n                        cdf(Normal(),(y[col] - ρ*y[row] - d/2) / σ))\n      end\n  end\n\n  yy = exp.(y .+ μ / (1 - ρ)) # center process around its mean (wbar / (1 - rho)) in new variable\n  # take exponential to get to a ln process\n\n  # renormalize. In some test cases the rows sum to something that is 2e-15\n  # away from 1.0, which caused problems in the MarkovChain constructor\n  Π = Π./sum(Π, dims = 2)\n\n  return Π, yy\nend\n\ntauchen (generic function with 3 methods)\n\n\n\nΠ, ϵ_vec = tauchen(100, E.ρϵ, E.σϵ)\n\n([0.2355397082043846 0.0398220365205688 … 0.0 0.0; 0.2035639923092491 0.03686888293414207 … 0.0 0.0; … ; 4.784202347164862e-30 1.4778853656337744e-29 … 0.03686888293414202 0.20356399230924901; 1.3827115757372961e-30 4.347177100735715e-30 … 0.039822036520568704 0.23553970820438463], [0.4908675431085489, 0.4979749102029403, 0.5051851862545925, 0.5124998613024105, 0.5199204469598645, 0.5274484767273707, 0.5350855063091983, 0.5428331139349621, 0.5506929006857724, 0.5586664908251056  …  1.7899766970506505, 1.8158941194896647, 1.84218680535361, 1.8688601881549594, 1.8959197800790755, 1.9233711731233287, 1.9512200402527142, 1.9794721365721941, 2.0081333005160213, 2.0372094550542794])\n\n\n\n\n6.2 Functions\nExporter fixed cost (just for illustration, we won’t use this function)\n\nfunction expfixcost(X1::Int, X2::Int; E = E)\n    #X1 and X2 are 0, 1 variables\n    f = (1-X1) * E.fE * X2 + X1 * E.fC * X2;\n    return f\nend\n\nexpfixcost (generic function with 1 method)\n\n\nFirm’s profit function\n\nfunction profit(X, Evalue, ξ_idx, P, Pstar; E= E, Qvalue = E.Q, τ = 0.0)\n    (;θ, Cstar, αn, αk, w, r, ξ ) = E\n    M = (1.0 + X * ξ[ξ_idx]^(1.0 - θ)* (1.0 - τ)^θ * ((Qvalue * Pstar/ P)^θ) * Cstar)^(1.0/θ) * P * Evalue\n    a = αn * (θ - 1.0)/θ\n    b = αk * (θ - 1.0)/θ\n    Profit = (1.0-a-b) * M^(1.0/(1.0-a-b)) * (a/w)^(a/(1.0-a-b)) * (b/r)^(b/(1.0-a-b))\n    \n    return Profit\nend\n\nprofit (generic function with 1 method)\n\n\nSales at domestic and foreign markets:\n\nfunction sales(X, Evalue, ξ_idx, P, Pstar; E= E, Qvalue = E.Q, τ = 0.0)\n    (;θ, Cstar, αn, αk, w, r, ξ) = E\n    M = (1.0 + X * ξ[ξ_idx]^(1.0 - θ)* (1.0 - τ)^θ * ((Qvalue * Pstar/ P)^θ) * Cstar)^(1.0/θ) * P * Evalue\n    a = αn * (θ - 1.0)/θ\n    b = αk * (θ - 1.0)/θ\n    con = ξ[ξ_idx]^θ * (1.0 - τ)^(-θ) * (P/ (Qvalue * Pstar))^θ/Cstar\n    sales_do = (con/(X * ξ[ξ_idx] + con))^((θ-1.0)/θ) * P * Evalue * M^((a + b)/(1.0-a-b)) * (a/w)^(a/(1.0-a-b)) * (b/r)^(b/(1.0-a-b))\n    if X == 0\n        sales_fo = 0.0\n    else\n        sales_fo = (1.0-τ) * (1.0/(ξ[ξ_idx] + con))^((θ-1.0)/θ) * Qvalue * Pstar * Cstar^(1.0/θ) * Evalue * M^((a + b)/(1.0-a-b)) * (a/w)^(a/(1.0-a-b)) * (b/r)^(b/(1.0-a-b)) # foreign sales net of tariffs\n    end\n    return [sales_do, sales_fo]\nend\n\nsales (generic function with 1 method)\n\n\nTake a try on the functions. Again . is used for broadcasting and Ref() is used to fix inputs.\n\nprofit.([1, 0], [1.2, 0.8], [1, 1], Ref(0.1), Ref(0.2))\n\n2-element Vector{Float64}:\n 0.0860809333951814\n 0.0025658309095826434\n\n\n\nreduce(hcat, sales.([1, 0], [1.2, 0.8], [1, 1], Ref(1.0), Ref(1)))'\n\n2×2 adjoint(::Matrix{Float64}) with eltype Float64:\n 9742.14  1040.57\n 1282.92     0.0\n\n\nIndividual prices (\\(p_j\\) and \\(p_j^*\\))\nThe function defined here needs to be modified to incorporate cases where \\(\\alpha_n + \\alpha_k \\neq 1\\).\n\nfunction ind_price(X, Evalue, ξ_idx; E = E, Qvalue = E.Q, τ = 0.0)\n    (;θ, αn, αk, w, r, ξ) = E\n    MC = Evalue^(θ/(1.0 - θ)) * (w/αn)^αn * (r/αk)^αk\n    do_p = (θ/(θ - 1.0)) * MC\n    if X == 0\n        fo_p = -1.0 # make foreign price negative for non-exporters\n    else\n        fo_p = (θ/(θ - 1.0)) / (1.0 - τ) * ξ[ξ_idx] * MC/Qvalue\n    end\n    return [do_p, fo_p]\nend\n\nind_price (generic function with 1 method)\n\n\n\nind_p_mat = reduce(hcat, ind_price.([0 1], [1.0 1.2], [1 1]))'[:,1]\nind_pstar_mat = reduce(hcat, ind_price.([0 1], [1.0 1.2], [1 1]))'[:,2]\n\n2-element Vector{Float64}:\n -1.0\n  0.16104839447304678\n\n\nAggregate price \\(P\\) and \\(P^*\\)\n\nfunction agg_price(ind_p_mat, ind_pstar_mat; θ = E.θ)\n    P = (sum(ind_p_mat.^(1.0 - θ)))^(1.0/(1.0 - θ))\n    Pstar = ((sum(ind_pstar_mat[ind_pstar_mat .&gt;= 0]).^(1.0 - θ)))^(1.0/(1.0 - θ))\n    return P, Pstar\nend\n\nagg_price (generic function with 1 method)\n\n\n\nagg_price(ind_p_mat, ind_pstar_mat)\n\n(0.09250365544979017, 0.16104839447304678)\n\n\n\n\n6.3 Solve the firm’s problem\nSolving the model is rather standard except that I chose to separate out the problem of a non-exporter and an exporter with either exporting technology \\(\\xi\\).\n\nfunction solve_exporter_fixQ(E, P, Pstar, τ; vNX0 = zeros(E.grids, 1), vEX0 = zeros(E.grids, length(E.ξ)), tol = 1e-6, Q = E.Q)\n    # unpacking\n    (; R, fE, fC, ξ, Ξ, fE, fC, ρϵ, σϵ, grids) = E\n\n    vNX1 = similar(vNX0) # to store value of non-exporters\n    vEX1 = similar(vEX0) # to store value of exporters\n    polNX = similar(vNX0, Int) # to store policy of non-exporters\n    polEX = similar(vEX0, Int) # to store policy of exporters\n\n    REvNX = similar(vNX0) # expected value if choosing not to export\n    REvEX_NX = similar(vNX0) # expected value if choosing to export as a non-exporter\n    REvEX_EX = similar(vEX0) # expected value if choosing to export as an exporter with different ξ's\n    \n    domesticm, ~ = sales(0, 1.0, 1, P, Pstar; E = E, Qvalue = Q, τ = 0.0) # sales of a non-exporting medium firm\n\n    # productivity process\n    Π, ϵ_vec = tauchen(grids, ρϵ, σϵ)\n\n    # Precompute the profits for firms at all states \n    proNX = profit.(zeros(Int, grids, 1), ϵ_vec, ones(Int, grids, 1), Ref(P), Ref(Pstar); E= E, Qvalue = Q, τ = τ)\n    # input for ξ doesn't matter\n\n    proEX = similar(vEX0)\n    for i in 1:length(ξ)\n        proEX[:,i] = profit.(ones(Int, grids, 1), ϵ_vec, i.*ones(Int, grids, 1), Ref(P), Ref(Pstar); E= E, Qvalue = Q, τ = τ)\n    end\n\n    # VFI\n    iter = 0\n    while true\n        distance = zero(eltype(vNX0))\n        iter += 1 \n\n        # Precompute RE[V(.)|X, ϵ, ξ] for better performance\n        REvNX = R * Π * vNX0\n        REvEX_NX = R * Π * vEX0[:,1] # enters with entry export technology\n        REvEX_EX = R * Π * vEX0 * Ξ'\n\n        vNX1 = max.((proNX + REvNX), (proNX .- fE * domesticm + REvEX_NX))\n        vEX1 = max.((proEX .+ REvNX), (proEX .- fC * domesticm + REvEX_EX))\n\n        distance = max(maximum(abs.(vNX1 - vNX0)), maximum(abs.(vEX1 - vEX0)))\n\n        (distance &lt; tol || iter == 2000) && break\n        vNX0 .= vNX1\n        vEX0 .= vEX1\n    end\n\n    # get policy\n    polNX .= 0\n    polEX .= 0\n    for i in 1:grids\n        if (REvNX[i]) &lt; (- fE * domesticm + REvEX_NX[i])\n            polNX[i] = 1\n        end\n        for j in 1:length(ξ)\n            if (REvNX[i]) &lt; (- fC * domesticm + REvEX_EX[i,j])\n                polEX[i,j] = 1\n            end\n        end\n    end\n\n    return (vNX = vNX1, vEX = vEX1, polNX = polNX, polEX = polEX, Π = Π, ϵ_vec =ϵ_vec, iter = iter)\n\nend\n\nsolve_exporter_fixQ (generic function with 1 method)\n\n\n\n@time begin\n    sol_try = solve_exporter_fixQ(E, 0.1, 0.1, 0.0);\nend\n\n  0.230753 seconds (373.87 k allocations: 31.695 MiB, 3.40% gc time, 65.36% compilation time)\n\n\n(vNX = [0.43476592047069756; 0.43593320133695335; … ; 1.4557350993346505; 1.4970252086816265;;], vEX = [0.43478975561531463 0.4348412512981293; 0.43595881274862097 0.4360141460454338; … ; 1.5042340280033728 1.641819695915486; 1.547864034400909 1.6930444062885848], polNX = [0; 0; … ; 1; 1;;], polEX = [0 0; 0 0; … ; 1 1; 1 1], Π = [0.2355397082043846 0.0398220365205688 … 0.0 0.0; 0.2035639923092491 0.03686888293414207 … 0.0 0.0; … ; 4.784202347164862e-30 1.4778853656337744e-29 … 0.03686888293414202 0.20356399230924901; 1.3827115757372961e-30 4.347177100735715e-30 … 0.039822036520568704 0.23553970820438463], ϵ_vec = [0.4908675431085489, 0.4979749102029403, 0.5051851862545925, 0.5124998613024105, 0.5199204469598645, 0.5274484767273707, 0.5350855063091983, 0.5428331139349621, 0.5506929006857724, 0.5586664908251056  …  1.7899766970506505, 1.8158941194896647, 1.84218680535361, 1.8688601881549594, 1.8959197800790755, 1.9233711731233287, 1.9512200402527142, 1.9794721365721941, 2.0081333005160213, 2.0372094550542794], iter = 375)\n\n\nFind the cutoff productivity levels\n\nfunction find_cutoff_prod(sol)\n    (; polNX, polEX, ϵ_vec) = sol\n    cutoff_ϵ = zeros(eltype(ϵ_vec), size(polEX)[2] + 1)\n    for i in 1:size(polNX)[1]\n        if polNX[i] == 1\n            cutoff_ϵ[1] = ϵ_vec[i]\n            break\n        end\n    end\n    \n    for j in 1:size(polEX)[2]\n        for i in 1:size(polEX)[1]\n            if polEX[i, j] == 1\n                cutoff_ϵ[j + 1] = ϵ_vec[i]\n                break # only break out of the inner loop\n            end\n        end\n    end\n\n    return cutoff_ϵ\nend\n\nfind_cutoff_prod (generic function with 1 method)\n\n\nNow we can find out the cutoff levels for the productivity \\(\\epsilon\\) for different export status \\(X\\) and export technology \\(\\xi\\) .\n\nfind_cutoff_prod(sol_try)\n\n3-element Vector{Float64}:\n 2.0081333005160213\n 1.6658337302526012\n 1.342716960827644\n\n\nThis says that among different states, non-exporter needs the highest draw of productivity to become an exporter and the exporter with highest \\(\\xi\\) will not go back to a non-exporter only if the productivity falls very low.\n\n\n6.4 Stationary distribution for firms given prices\n\nfunction compute_stationary_distribution(sol;E = E, tol = 1e-8, pdf_0 = fill(1.0 / (size(sol_try.vEX)[1] * (size(sol_try.vEX)[2] + 1))  , (size(sol_try.vEX)[1] , (size(sol_try.vEX)[2] + 1))))\n\n    (; polEX, polNX, vEX, vNX) = sol\n\n    (; Ξ, ρϵ, σϵ, grids) = E\n\n    # productivity process\n    Π, ϵ_vec = tauchen(grids, ρϵ, σϵ)\n    \n    pdf_1 = similar(pdf_0)\n\n    iter = 0\n    while true \n        fill!(pdf_1, zero(eltype(pdf_0)))\n        iter +=1\n\n        for i in 1:grids\n            for iprime in 1:grids\n                pdf_1[iprime,1] += (1 - polNX[i]) * pdf_0[i, 1] * Π[i, iprime]\n                pdf_1[iprime,2] += polNX[i] * pdf_0[i, 1] * Π[i, iprime]\n                for j in 1:size(Ξ)[1]\n                    pdf_1[iprime,1] += (1 - polEX[i,j]) * pdf_0[i, j + 1] * Π[i, iprime]\n                    for jprime in 1:size(Ξ)[1]\n                        pdf_1[iprime,jprime + 1] += polEX[i, j] * Ξ[j, jprime] * Π[i, iprime] * pdf_0[i ,j + 1] \n                    end\n                end\n            end\n        end\n        \n        distance = zero(eltype(pdf_0))\n        for (a, b) in zip(pdf_0, pdf_1)\n            distance = max(abs(a - b), distance)\n        end\n        \n        (distance &lt; tol || iter == 2000) && break \n        pdf_0 .= pdf_1\n    end\n    pdf_1 = pdf_1\n    return pdf_1, iter\nend\n\ncompute_stationary_distribution (generic function with 1 method)\n\n\n\n@time begin\n    pdf_stationary, _ = compute_stationary_distribution(sol_try)\nend\n\n  0.082796 seconds (104.75 k allocations: 6.769 MiB, 93.08% compilation time)\n\n\n([0.0011570502171430112 1.134093337951658e-22 1.3041790003368101e-21; 0.0002998023372470313 2.0936653656776906e-22 2.4076453556327584e-21; … ; 0.0002164516680350745 7.504147054101457e-5 8.309198670945468e-6; 0.0007172049628995025 0.00040114335735656076 3.870189688694214e-5], 49)\n\n\nCheck if it is still a distribution:\n\nsum(pdf_stationary)\n\n1.0000000000000007\n\n\nLook at the shares of exporters at each states.\n\nNX_share, EX_low_share, EX_high_share = sum(pdf_stationary, dims = 1)\nprintln(\"The share of nonexporters is $NX_share, the share of exporters with low technology is $EX_low_share, the share of exporters with high technology is $EX_high_share, given prices in the stationary distribution, \")\n\nThe share of nonexporters is 0.9966134925743302, the share of exporters with low technology is 0.002694855795406125, the share of exporters with high technology is 0.000691651630264524, given prices in the stationary distribution, \n\n\n\n\n6.5 Stationary prices\nNow we have all the pieces ready for the stationary prices.\nThe algorithm goes as follows:\n\nFix the number of firms N\nStart from guesses of \\(P_0\\) and \\(P^*_0\\)\nFor each \\(P_t\\) and \\(P^*_t\\) solve the firm’s problem given prices.\nCompute invariant distribution \\(\\lambda(X, \\epsilon, \\xi)\\)\nGet new individual firm prices according to first order conditions of firms.\nGet the updated aggregate prices \\(P'_t\\) and \\(P'^{*}_t\\).\nGuess \\(P_{t + 1} = ϵ P'_t + (1 - ϵ) P_t\\) and \\(P^*_{t + 1} = ϵ P'^*_t + (1 - ϵ) P^*_t\\). Go back to the second step and iterate until convergence.\n\n\nfunction compute_stationary_equilibrium(E; P0=0.1, Pstar0=0.1, tol=1e-5, ϵ=0.2, τ=0.0, N=2000)\n    (; Ξ, ρϵ, σϵ, grids, θ) = E\n\n    Π, ϵ_vec = tauchen(grids, ρϵ, σϵ) # discretized AR1\n\n    ind_p_mat = zeros(grids, size(Ξ)[1] + 1)\n    ind_pstar_mat = zeros(grids, size(Ξ)[1] + 1)\n\n    sol = solve_exporter_fixQ(E, P0, Pstar0, τ) #solve the DP under some guesses of aggregate prices\n    pdf, ~ = compute_stationary_distribution(sol) #compute the stationary distribution using the policy\n\n    for i in 1:grids\n        ind_p_mat[i, 1], ind_pstar_mat[i, 1] = ind_price(0, ϵ_vec[i], 1; τ=τ) .* pdf[i, 1] #compute individual prices for non-exporting firms\n        for j in 1:size(Ξ)[1]\n            ind_p_mat[i, j+1], ind_pstar_mat[i, j+1] = ind_price(1, ϵ_vec[i], j; τ=τ) .* pdf[i, j+1] #compute individual prices for exporters\n        end\n    end\n\n    Pprime1 = (sum(ind_p_mat.^(1.0 - θ) .*pdf .* N ))^(1.0/(1.0 - θ)) # compute aggregate domestic price\n    temp = ind_pstar_mat.^(1.0 - θ) .*pdf .* N \n    Pstarprime1 = (sum(temp[temp .&gt; 0.0]))^(1.0/(1.0 - θ)) # compute aggregate foreign price of exporters\n\n    P1 = ϵ * Pprime1 + (1.0 - ϵ) * P0 # update rule for domestic price\n    Pstar1 = ϵ * Pstarprime1 + (1.0 - ϵ) * Pstar0 # update rule for foreign price\n\n    P0 = copy(P1)\n    Pstar0 = copy(Pstar1)\n\n    iter = 0\n    while true #put the steps above in iteration and iterate until convergence\n        distance = zero(eltype(P0))\n        iter += 1\n\n        sol = solve_exporter_fixQ(E, P0, Pstar0, τ)\n        pdf, ~ = compute_stationary_distribution(sol)\n\n        for i in 1:grids\n            ind_p_mat[i, 1], ind_pstar_mat[i, 1] = ind_price(0, ϵ_vec[i], 1; τ=τ)\n            for j in 1:size(Ξ)[1]\n                ind_p_mat[i, j+1], ind_pstar_mat[i, j+1] = ind_price(1, ϵ_vec[i], j; τ=τ)\n            end\n        end\n\n        Pprime1 = (sum(ind_p_mat.^(1.0 - θ) .*pdf .* N ))^(1.0/(1.0 - θ))\n        temp = ind_pstar_mat.^(1.0 - θ) .*pdf .* N \n        Pstarprime1 = (sum(temp[temp .&gt; 0.0]))^(1.0/(1.0 - θ))\n\n        P1 = ϵ * Pprime1 + (1.0 - ϵ) * P0\n        Pstar1 = ϵ * Pstarprime1 + (1.0 - ϵ) * Pstar0\n\n        distance = max(abs(P1 - P0), abs(Pstar1 - Pstar0))\n        (distance &lt; tol || iter == 2000) && break\n\n        P0 = copy(P1)\n        Pstar0 = copy(Pstar1)\n    end\n\n    return P1, Pstar1, ind_p_mat, ind_pstar_mat, sol, pdf, iter\nend\n\ncompute_stationary_equilibrium (generic function with 1 method)\n\n\n\n@time begin\n    P_rce, Pstar_rce, ind_p_mat_rce, ind_pstar_mat_rce, sol_rce, pdf_rce, iter_rce = compute_stationary_equilibrium(E)\nend\n\n  0.531602 seconds (407.32 k allocations: 100.759 MiB, 3.33% gc time, 15.25% compilation time)\n\n\n(0.015952055070459048, 0.022740007481633726, [0.30768588844713596 0.30768588844713596 0.30768588844713596; 0.30220639279011097 0.30220639279011097 0.30220639279011097; … ; 0.052883757118820714 0.052883757118820714 0.052883757118820714; 0.05194196443888268 0.05194196443888268 0.05194196443888268], [-1.0 0.4922974215154175 0.36922306613656314; -1.0 0.4835302284641776 0.3626476713481332; … ; -1.0 0.08461401139011314 0.06346050854258485; -1.0 0.08310714310221229 0.062330357326659216], (vNX = [2.6372572588229866e-5; 2.6534988291849078e-5; … ; 0.00019073299008637822; 0.0001965218456753796;;], vEX = [2.6387095351844466e-5 2.6418471692987123e-5; 2.6550593335722793e-5 2.6584307936684532e-5; … ; 0.00021410337885572679 0.00030581855386856; 0.00022130259243730753 0.00031748413001791764], polNX = [0; 0; … ; 1; 1;;], polEX = [0 0; 0 0; … ; 1 1; 1 1], Π = [0.2355397082043846 0.0398220365205688 … 0.0 0.0; 0.2035639923092491 0.03686888293414207 … 0.0 0.0; … ; 4.784202347164862e-30 1.4778853656337744e-29 … 0.03686888293414202 0.20356399230924901; 1.3827115757372961e-30 4.347177100735715e-30 … 0.039822036520568704 0.23553970820438463], ϵ_vec = [0.4908675431085489, 0.4979749102029403, 0.5051851862545925, 0.5124998613024105, 0.5199204469598645, 0.5274484767273707, 0.5350855063091983, 0.5428331139349621, 0.5506929006857724, 0.5586664908251056  …  1.7899766970506505, 1.8158941194896647, 1.84218680535361, 1.8688601881549594, 1.8959197800790755, 1.9233711731233287, 1.9512200402527142, 1.9794721365721941, 2.0081333005160213, 2.0372094550542794], iter = 37), [0.001156456913348616 4.7648692493456394e-8 5.456551019037757e-7; 0.00029942249071175333 3.057907732662415e-8 3.49267457951519e-7; … ; 9.407333285062188e-10 0.00016943495830104894 0.0001303664382126568; 9.523762527622712e-10 0.0006498609578498225 0.0005071883069169294], 35)\n\n\nJust look at the insane speed. Remember we didn’t do any parallelization and stuff. These are just efficient codes.\n\n\n6.6 Some results from the stationary equilibrium\nShare of firms in each state\n\nNX_share, EX_low_share, EX_high_share = sum(pdf_rce, dims = 1)\nprintln(\"The share of nonexporters is $NX_share, the share of exporters with low technology is $EX_low_share, the share of exporters with high technology is $EX_high_share, in the stationary distribution, \")\n\nThe share of nonexporters is 0.4526148278973261, the share of exporters with low technology is 0.3046577047390891, the share of exporters with high technology is 0.24272746736358444, in the stationary distribution, \n\n\n\nplot(ϵ_vec, sol_rce.vNX, xlabel = \"productivity\", ylabel = \"value\", label = \"Non-exporters\")\nplot!(ϵ_vec, sol_rce.vEX[:,1], label = \"Exporters with low export technology\")\nplot!(ϵ_vec, sol_rce.vEX[:,2], label = \"Exporters with high export technology\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfind_cutoff_prod(sol_rce)\n\n3-element Vector{Float64}:\n 1.1299695318784664\n 0.9239802788519544\n 0.7133227740212923\n\n\nGraphically:\n\nplot(ϵ_vec, sol_rce.polNX, xlabel = \"productivity\", ylabel = \"policy\", label = \"Non-exporters\", yticks = [0,1])\nplot!(ϵ_vec, sol_rce.polEX[:,1], label = \"Exporters with low export technology\")\nplot!(ϵ_vec, sol_rce.polEX[:,2], label = \"Exporters with high export technology\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(ϵ_vec, pdf_rce[:,1], xlabel = \"productivity\", ylabel = \"fraction\", label = \"Non-exporters\", title = \"Stationary distribution\")\nplot!(ϵ_vec, pdf_rce[:,2], label = \"Exporters with low export technology\")\nplot!(ϵ_vec, pdf_rce[:,3], label = \"Exporters with high export technology\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCutoff productivity for firms at different states.\nWe can collect some moments from the stationary distribution. They will be useful for estimations.\n\nfunction moments_fixedQ(P, Pstar, sol, pdf, ind_p_mat, ind_pstar_mat; E = E, Qvalue = E.Q, τ = 0.0)\n    (;ρϵ, σϵ, grids, ξ) = E\n\n    Π, ϵ_vec = tauchen(grids, ρϵ, σϵ)\n\n    # exporter share\n    exporter_share = sum(pdf, dims = 1)[1]\n\n    # starter rate\n    starter_rate = sol.polNX⋅pdf[:,1]\n\n    # stopper rate (should be same as starter rate under stationary equilibrium)\n    stopper_rate = (1 .- sol.polEX)⋅pdf[:,2:3]\n    \n    # exporter sales ratio conditional on exporting\n    sales_EX_do = zeros(eltype(P), grids, length(ξ))\n    sales_EX_fo = similar(sales_EX_do)\n    accum = zero(eltype(P))\n    for i in 1:grids\n        for j in 1:length(ξ)\n               sales_EX_do[i, j], sales_EX_fo[i, j]  = sales(1, ϵ_vec[i], j, P, Pstar; E= E, Qvalue = Qvalue, τ = τ)\n               accum += sales_EX_fo[i, j]/(sales_EX_do[i, j] + sales_EX_fo[i, j]) * pdf[i, j+1]/(1.0-exporter_share )\n        end\n    end\n\n    # average export price (in domestic currency) and domestic price ratio conditional on exporting\n    fo_price_ratio = sum(Qvalue.*ind_pstar_mat[:,2:3]./ind_p_mat[:,2:3].*pdf_rce[:,2:3])/(1.0-exporter_share)\n\n    return exporter_share, starter_rate, stopper_rate, accum, fo_price_ratio \nend\n\nmoments_fixedQ (generic function with 1 method)\n\n\n\nmoments_fixedQ(P_rce, Pstar_rce, sol_rce, pdf_rce, ind_p_mat_rce, ind_pstar_mat_rce)\n\n(0.19266736559312644, 0.01391198296999171, 0.013911817889600284, 0.46736658942224996, 1.4059666732401315)\n\n\nNote that by optimality condition of an exporting firm. \\(Q p_j^* /p_j = \\frac{\\xi}{1 - \\tau}\\), so the last moment, the average export price ratio conditional on exporting can also be computed from the the parameter values of \\(\\xi\\) and \\(\\tau\\) and the conditional fraction of exporters at different states of \\(\\xi\\). This moment can be interpreted as a premium for exporting.\nPlay with it and try with different parameters. How does the steady state equilibrium change? You might notice something odd when changing the tariff \\(\\tau\\), what drives the result?"
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html",
    "title": "Introduction to Julia on Computational Economics",
    "section": "",
    "text": "Xing Xu, University of Minnesota, 2024 Summer"
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html#overview",
    "href": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html#overview",
    "title": "Introduction to Julia on Computational Economics",
    "section": "Overview",
    "text": "Overview\nThe purpose of the class is to get you familiarize with some basic dynamic programming (DP) problems in economics and train you to be comfortable with coding them in Julia.\n\nWhy DP?\nDynamic programming is the key to solving modern economic models. Most macro models now are dynamic general equilibrium models. Knowing how to solve recursive models is in additional useful for searching, matching and even game theory (with sequential games).\n### Why Julia?\nJulia is the most modern tool for computations. It preserves the efficiency and clarity of coding while providing astounding speed. For comparison, c++ and Fortran are as fast but coding in them requires a lot more work as basically everything has to be built from scratch. Another extreme is Stata, which is super easy to code in (say, reg y x), but is absurdly slow and extremely limited in its applications (can’t even perform DP).\nJulia reaches a great balance and is perfectly suited for computational economics. You will see its beauty in action once you get comfortable with it.\n\n\nBackground knowledge\nI will list the backgrounds used that are considered as known. Additional knowledge will be specified when used.\nMath Preliminaries (undergraduate level knowledge is sufficient):\n\nLinear algebra\nReal analysis\nProbability theory\nBasic understanding of Markov Chains\nKnowledge of basic optimization theory will be a plus (Sundaram &lt;A First Course in Optimization Theory&gt;)\n\nEconomics: * Basic choice theory (Chapter 2 MWG) (Preferences, utility representation)\n\nBasic consumer theory (Chapter 3 MWG) (Utility maximization)\nBasic producer theory (Chapter 5 MWG)\nAn idea of Competitive Equilibrium and Welfare Theorems (Chapter 10 MWG)\n\nJulia (required reading): * Setup of Julia environment, Quantecon 1\n\nIntroduction to Julia Quantecon 2\nJulia Essentials Quantecon 3\nArrays, Tuples, Ranges, and Other Fundamental Types Quantecon 4\nBasic understanding is good: Introduction to Types and Generic Programming Quantecon 5\n\nOther good references:\nNotes: * Dirk Krueger’s Macroeconomic Theory notes\n\nTim Kehoe’s notes on Blackwell sufficient conditions\n\nBooks: * Adda & Cooper &lt;Dynamic economics&gt; (strongly recommend)\n\nStokey, Lucas with Prescott &lt;Recursive methods in economic dynamics&gt;\nLjungqvist & Sargent &lt;Recursive Macroeconomic Theory&gt;\nHeer & Maußner &lt;Dynamic General Equilibrium Modeling&gt;\nStachurski &lt;Economic Dynamics: Theory and Computation (Second Edition)&gt;"
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html#lecture-1-julia-fundamentals",
    "href": "Julia/Lectures/Julia_Lec_1_Compecon_Xing_Xu.html#lecture-1-julia-fundamentals",
    "title": "Introduction to Julia on Computational Economics",
    "section": "Lecture 1: Julia Fundamentals",
    "text": "Lecture 1: Julia Fundamentals\nThis acts as a review for Lecture 1.1 - 1.5 for Julia on Quantecon. I will only focus on stuffs we will use for the following lectures. This lecture accompanies a succinct assignment to get you familiarized with writing in the language and playing with it on the Jupyter Notebook.\n\n1.1 Packages and Projects\nWorking with packages is of vital importance in Julia. We use “LinearAlgebra” for matrix operations, “Statistics” for different distributions, “Plots” for, obviously, plots, etc. One thing you always need to keep in mind is how to download some packages.\nAfter downloading the essential packages you need, you have developed something called an “environment”. For compatibility, version control and reproducibility reasons, one often wants to create individual environments for different files. “Projects” manages each individual local environments. It will create a project repository that usually contains a “Project.toml”, a “Manifest.toml” and your code files.\n\n\n1.1.1 Packages and Projects using the Pkg API\nNow what are these exactly? Put this notebook into an individual folder and run the following lines.\n\nimport Pkg\n\n\nPkg.activate(\".\")  # Activate a local environment \nPkg.add(\"LinearAlgebra\") # add the package \"LinearAlgebra\"\n\nYou should find two extra files in the folder. A “Project.toml” and a “Manifest.toml”. Double click on each of them in the folder or just click on them in the editor in Vscode. Take a look. What are in them?\nThe “Project.toml” file contains the name of the package, its unique UUID, its version, the authors and potential dependencies.\nThe “Manifest.toml” file shows the information for the resulting dependencies of the added packages.\nYou can check the status of your package using the following code.\n\nPkg.status()\n\nStatus `~/Documents/Teaching/Julia course 2024/Julia/Lectures/Project.toml`\n  [37e2e46d] LinearAlgebra\n\n\nSometimes you want to run other people’s code. Say you have stored some people’s project into a folder on your laptop (you just need the “Project.toml” file). To activate the exact environment, the following code will automatically download all packages and their dependencies of the same version and make it the current environment.\n\nPkg.instantiate()\n\n\n\n1.1.2 Packages and Projects using REPL\nAll the above operations can also be done using REPL (Read-Evaluate-Print-Loop).\nTo activate REPL in Vscode, press CTRL + Shift + P in Windows or CMD + Shift + P in OS, then type “start REPL” in the command palette and click on it.\nNow the Terminal should pop up and you should see “julia&gt;”. This indicates we have successfully started the REPL.\nNext type ] to enter the package mode. It should write something like “(Julia course 2023 Summer) pkg&gt;”. To exit, simply press delete.\nWith the package mode, we can perform the same functionality as before and more.\nactivate. activate the project at the current directory.\nst shows the status.\nadd LinearAlgebra add the LinearAlgebra package to the local environment.\ninstantiate automatically downloads all the missing dependencies from the particular “Project.toml”.\n\n\n1.2 Introductory example for using packages\nWe will see a lot more packages and their usages along the way. For the moment, I will show some basic functionalities of the LinearAlgebra package as an example. For more information, check here.\nFirst, you need to call the Package with “using”.\n\nusing LinearAlgebra\n\nDefine a matrix:\n\nA = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nA couple things you can do. The codes explain themselves.\n\ntr(A)\n\n15\n\n\n\ndet(A)\n\n-9.516197353929915e-16\n\n\n\ninv(A)\n\n3×3 Matrix{Float64}:\n  3.15252e15  -6.30504e15   3.15252e15\n -6.30504e15   1.26101e16  -6.30504e15\n  3.15252e15  -6.30504e15   3.15252e15\n\n\n\nnorm(A)\n\n16.881943016134134\n\n\n\neigvals(A)\n\n3-element Vector{Float64}:\n -1.1168439698070434\n -8.582743335036247e-16\n 16.11684396980703\n\n\n\neigvecs(A)\n\n3×3 Matrix{Float64}:\n -0.78583     0.408248  -0.231971\n -0.0867513  -0.816497  -0.525322\n  0.612328    0.408248  -0.818673\n\n\nI represents an identity matrix of any size. For example,\n\nA * I\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nThe dot product can also be calculated.\n\ndot(A, I) # Same as tr(A)\n\n15\n\n\n\nA ⋅ I\n#⋅ is typed by \"\\cdot\" then press \"tab\"\n\n15\n\n\nAsolves Ax = B. Note that this is strictly preferred to inv(A)*B. Never do the latter.\n\nB = [1, 2, 3]\nA\\B\n# or B/A\n\n3-element Vector{Float64}:\n -0.23333333333333334\n  0.46666666666666673\n  0.09999999999999994\n\n\nElement-wise operations are done through . (more discussions on this next lecture).\n\nA .+ 3\n\n3×3 Matrix{Int64}:\n  4   5   6\n  7   8   9\n 10  11  12\n\n\nYou can also perform factorizations on the matrices.\n\nlu(A)\n\nLU{Float64, Matrix{Float64}}\nL factor:\n3×3 Matrix{Float64}:\n 1.0       0.0  0.0\n 0.142857  1.0  0.0\n 0.571429  0.5  1.0\nU factor:\n3×3 Matrix{Float64}:\n 7.0  8.0        9.0\n 0.0  0.857143   1.71429\n 0.0  0.0       -1.58603e-16\n\n\n\nqr(A)\n\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.123091   0.904534   0.408248\n -0.492366   0.301511  -0.816497\n -0.86164   -0.301511   0.408248\nR factor:\n3×3 Matrix{Float64}:\n -8.12404  -9.60114   -11.0782\n  0.0       0.904534    1.80907\n  0.0       0.0        -8.88178e-16\n\n\n\n# Cholesky decomposition doesn't work here, which creates an error\ncholesky(A)\n\nPosDefException: PosDefException: matrix is not Hermitian; Cholesky factorization failed.\n\n\nNot surprisingly, we can also do vector operations.\n\nv1 = [1, 1, 3]\nv2 = [2, 2, 3]\n\n3-element Vector{Int64}:\n 2\n 2\n 3\n\n\n\nv1 + v2 \n# same as v1 .+ v2\n\n3-element Vector{Int64}:\n 3\n 3\n 6\n\n\nDot products for vectors are same as for matrices, except for vectors we can simply do:\n\nv1' * v2 #' for transpose\n\n13\n\n\n\ndot(v1, v2)\n\n13\n\n\n\nv1 ⋅ v2\n\n13\n\n\n\nα = 0.3\nα * 2\n\n0.6\n\n\n\n\n1.2 Functions\nIn Julia functions can be built in line or specifically calling function.\n\nf(x, y) =  x^y\nf(2,2)\n\n4\n\n\n\n#Or\nfunction f(x, y)\n    return x^y\nend\nf(2,2)\n\n4\n\n\nIf you want to input an array into f, it gives an error as it is not clear what you mean.\n\nv1 = [1, 2, 3]\nf(v1, 2)\n\nMethodError: MethodError: no method matching ^(::Vector{Int64}, ::Int64)\n\nClosest candidates are:\n  ^(!Matched::Union{AbstractChar, AbstractString}, ::Integer)\n   @ Base strings/basic.jl:733\n  ^(!Matched::Diagonal, ::Integer)\n   @ LinearAlgebra ~/Applications/Julia-1.9.3.app/Contents/Resources/julia/share/julia/stdlib/v1.9/LinearAlgebra/src/diagonal.jl:208\n  ^(!Matched::Diagonal, ::Real)\n   @ LinearAlgebra ~/Applications/Julia-1.9.3.app/Contents/Resources/julia/share/julia/stdlib/v1.9/LinearAlgebra/src/diagonal.jl:207\n  ...\n\n\nYou can modify this by defining the function specifically for vectors.\n\nfunction g(x,y)\n    z = similar(x) # similar() generates an size/matrix of the same size and type as the input\n    for i in eachindex(x)\n        z[i] = x[i]^y\n    end\n    return z\nend\n@show g(v1, 2)\n\ng(v1, 2) = [1, 4, 9]\n\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nOr in a more concise fashion,\n\ng(x,y) = [z^y for z in x] # or g(x,y) = [x[i]^y for i in eachindex(x)]\ng(v1, 2)\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nInstead, you can also apply “.” to broadcast the function onto every element of v1. This comes very handy in a lot of cases.\n\nf(x, y) =  x^y\nf.(v1, 2)\n# should write f.(v1, Ref(2)) for rigidity. You can think of Ref() as fixing the input here.\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\n\n\n1.3 Types (a short summary)\nTypes are incredibly important for coding in Julia. It matters for the correctness, consistency and efficiency of the codes. However, my past experience told me that it is almost impossible to learn this from pure memorization. You have to actually work with them and think along the way. Here I will just let you see some commonly used types and give a simple example about declaration of types.\nFor more info on types, check the documentation.\nThe default type for (simple) integers is Int64\n\n@show typeof(1)\n\ntypeof(1) = Int64\n\n\nInt64\n\n\nThe default type for (simple) floats is Float64\n\n@show typeof(1.0) #putting \".0\" after 1 make the Julia compiler infer it is a float\n\ntypeof(1.0) = Float64\n\n\nFloat64\n\n\nThe default type for strings is String\n\n@show typeof(\"string\")\n\ntypeof(\"string\") = String\n\n\nString\n\n\nType of Boolean (true of false) variables.\n\n@show typeof(true)\n@show typeof(false)\n\ntypeof(true) = Bool\ntypeof(false) = Bool\n\n\nBool\n\n\n\n# Note that sometimes we use the feature that true equals to 1 and false equals to 0\n@show true == 1\n@show false == 0\n\ntrue == 1 = true\nfalse == 0 = true\n\n\ntrue\n\n\nWhat are the type of arrays?\n\n@show typeof([1, 2, 3])\n\ntypeof([1, 2, 3]) = Vector{Int64}\n\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n@show typeof([1.0, 2.0, 3.0])\n\ntypeof([1.0, 2.0, 3.0]) = Vector{Float64}\n\n\n\nVector{Float64} (alias for Array{Float64, 1})\n\n\n\nHow about the type of [1, 2.0, 3.0], where the first entry is an integer?\n\n@show typeof([1, 2.0, 3.0])\n\ntypeof([1, 2.0, 3.0]) = Vector{Float64}\n\n\n\nVector{Float64} (alias for Array{Float64, 1})\n\n\n\nNote that it returns a vector of floats. This is subtle but keep in mind that sometimes these type of inconsistencies can create problems. This also indicates that the type within an array is unified, to find the type of the elements in an array. We commonly use the following eltype().\n\n@show eltype([1.0, 2.0, 3.0])\n# differentiate with typeof([1.0, 2.0, 3.0])\n\neltype([1.0, 2.0, 3.0]) = Float64\n\n\nFloat64\n\n\nTuples are constructed using small brackets. We can also check the type of a tuple, which is different from the behavior of an array (vector).\n\n@show typeof((1, 2.0, 3.0, \"string\"))\n\ntypeof((1, 2.0, 3.0, \"string\")) = Tuple{Int64, Float64, Float64, String}\n\n\nTuple{Int64, Float64, Float64, String}\n\n\nNow let’s look at the function in 1.2 again, which is\n\nfunction g(x,y)\n    z = similar(x)\n    for i in eachindex(x)\n        z[i] = x[i]^y\n    end\n    return z\nend\n@show g(v1, 2)\n\ng(v1, 2) = [1, 4, 9]\n\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nIn practice, sometimes we want to specify the types of inputs, which can be down using ::.\n\nfunction g(x :: Vector{Int64},y :: Int64)\n    z = similar(x)\n    for i in eachindex(x)\n        z[i] = x[i]^y\n    end\n    return z\nend\n@show g(v1, 2)\n\ng(v1, 2) = [1, 4, 9]\n\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\n\ng([2], 2)\n\n1-element Vector{Int64}:\n 4\n\n\n\ng([2.0], 2)\n\n1-element Vector{Float64}:\n 4.0\n\n\n\ng(2.0, 2) # this doesn't work \n\nMethodError: MethodError: no method matching similar(::Float64)\n\nClosest candidates are:\n  similar(!Matched::Union{Adjoint{T, var\"#s972\"}, Transpose{T, var\"#s972\"}} where {T, var\"#s972\"&lt;:(AbstractVector)})\n   @ LinearAlgebra ~/Applications/Julia-1.9.3.app/Contents/Resources/julia/share/julia/stdlib/v1.9/LinearAlgebra/src/adjtrans.jl:329\n  similar(!Matched::Union{Adjoint{T, var\"#s972\"}, Transpose{T, var\"#s972\"}} where {T, var\"#s972\"&lt;:(AbstractVector)}, !Matched::Type{T}) where T\n   @ LinearAlgebra ~/Applications/Julia-1.9.3.app/Contents/Resources/julia/share/julia/stdlib/v1.9/LinearAlgebra/src/adjtrans.jl:330\n  similar(!Matched::Union{Adjoint{T, S}, Transpose{T, S}} where {T, S})\n   @ LinearAlgebra ~/Applications/Julia-1.9.3.app/Contents/Resources/julia/share/julia/stdlib/v1.9/LinearAlgebra/src/adjtrans.jl:333\n  ...\n\n\nHowever this sometimes creates confusions. We can input a matrix and still get out results:\n\nm1 = [2 2; 2 2]\ng(m1, 2)\n\n2×2 Matrix{Int64}:\n 4  4\n 4  4\n\n\nThis is why Quantecon stresses the importance for generic programming. See Quantecon 5 and Quantecon 6 for details. In short, you should avoid specifying particular types unless necessary and let your codes be compatible with different types.\n\nIn addition, keep in mind that you might specify some types without even knowing that you are doing so. One example is when you want to create a vector of 0 of type Int with length 5. It is tempted to use the built-in function zeros(5). Beware that this implicitly imposes that 0s are of type float.\n\nzeros(5)\n\n5-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\n\n\n@show eltype(zeros(5))\n\neltype(zeros(5)) = Float64\n\n\nFloat64\n\n\nSuppose you want the zeros with type int, one way is to use Int(). Again you need to broadcast it to each element.\n\nInt.(zeros(5))\n\n5-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n\n\nOr you can simply declare within zeros().\n\nzeros(Int,5)\n\n5-element Vector{Int64}:\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n1.4 Help?\nHow do you know you can do something as zeros(Int,5)? For most built-in functions, you can find it using Help from the REPL.\nIf you haven’t quitted Vscode, you are probably still in REPL from 1.1.\nIf you have, press CTRL + Shift + P in Windows or CMD + Shift + P in OS, then type “start REPL” in the command palette and click on it. You should again see “julia&gt;”, which indicates you are at REPL.\nNow simply typing ? enters into the help mode. You should see “help?&gt;”. Let’s try with the function zeros(). Type in the zeros or zeros() and press Return on your keyboard. You should see it shows the first input of function is “[T=Float64,]”, so unless otherwise specified, zeros() will generate an array with type “Float64”.\nHelp? is usually the first place to go if you encounter some built-in functions that you are not sure with. As we go, you will be introduced to a lot more of them.\n\n\n1.5 Macros (not Macroeconomics)\nMacros are a powerful tool in Julia. Macros are a type of metaprogramming (whatever this means). Just keep in mind that Macros start with an @ and take in general expressions instead of values as inputs. You can also check with help? for documentation of each Macro.\nI know it sounds a little absurd but knowing Macros well is a ticket to becoming a Julia master. For the moment, knowing a couple examples is sufficient.\nWe have used @show to show the output of a function. However, note that the inputs it can take is much more general. Say, it can take in the function itself.\n\n@show f\n\nf = f\n\n\nf (generic function with 1 method)\n\n\n\n@show f(1,2)\n\nf(1, 2) = 1\n\n\n1\n\n\n@time begin ... end works like tic ... tok in Matlab. It gives the duration of running the code between.\n\n@time begin\n    1 + 1\nend\n\n  0.000001 seconds\n\n\n2\n\n\nAnother example is @assert, which can be used for generating some statements if some conditions are not met.\n\nv1 = [1, 2, 3]\nv2 = [4, 5, 6]\n@assert v1 == v2 \"v1 is not equal to v2\"\n\nAssertionError: AssertionError: v1 is not equal to v2\n\n\n\n# no statement is generated if the condition is met\n@assert v1 == v1 \"v1 is not equal to v1\"\n\nA couple other useful Macros:\n@. Broadcasts . onto all arguments in one line.\n@inbounds Eliminates array bounds checking within expressions (efficiency).\n@inline Performs inline Maths (efficiency).\n@view Creates a data structure that acts like an array (it is a subtype of AbstractArray ), but the underlying data is actually part of another array. Often used to create a temporary array for efficiently.\nFor more, check here.\n\nj(x) = x^2\nv1 = [1,2,3]\nj.(v1)\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\n\n@. (j(v1) + 2)*2\n\n3-element Vector{Int64}:\n  6\n 12\n 22\n\n\n\n\n1.6 Miscellaneous facts about Julia (under construction)\nJulia has a particular behavior when assigning an array to another. What is your guess for the result of the following code?\n\nx = [3, 3]\ny = x\ny[1] = 6\n@show x\n\nx = [6, 3]\n\n\n2-element Vector{Int64}:\n 6\n 3\n\n\nOne might guess that the output will be “[3, 3]” as x is not changed. It is exactly what will happen in Python or Matlab. However, running it gives a seemingly surprising result.\n\nx = [3, 3]\ny = x\ny[1] = 6\n@show x\n\nx = [6, 3]\n\n\n2-element Vector{Int64}:\n 6\n 3\n\n\n\n# also\nx[2] = 6\n@show y\n\ny = [6, 6]\n\n\n2-element Vector{Int64}:\n 6\n 6\n\n\nThe reason is that on the background of Python or Matlab. When you run y = x, they create a copy for x in the storage and make it y. However in Julia, y = x simply assigns y to the same array that x is assigned to. That’s why changing either of them changes the other.\nThere are two alternative ways if you want the same behavior as in Python or Matlab.\n\nx = [3, 3]\ny = copy(x)\ny[1] = 6\n@show x\n\nx = [3, 3]\n\n\n2-element Vector{Int64}:\n 3\n 3\n\n\n\nx = [3, 3]\ny .= x #element-wise assignment dodges the problem\ny[1] = 6\n@show x\n\nx = [3, 3]\n\n\n2-element Vector{Int64}:\n 3\n 3\n\n\nEither one of them essentially makes y and x to be assigned to “different” arrays in your storage. If you know what a “pointer” is in c++, they now simply point to different places."
  },
  {
    "objectID": "Julia/Lectures/Julia_Lec_3_Compecon_Xing_Xu.html",
    "href": "Julia/Lectures/Julia_Lec_3_Compecon_Xing_Xu.html",
    "title": "Lecture 3: Root finding, Interpolation and Continuous optimization",
    "section": "",
    "text": "This lecture is inspired by a question from Minnesota Alumnus Kim Ruhl, who is a coding enthusiast who was new to Julia. In response, I provided this brief introduction to optimization in Julia.\nIn this lecture and the third assignment, we will build towards solving standard dynamic programming problems in economics with continuous optimization methods. If you recall from the second lecture, given capital \\(k\\), we search on a discrete grid of \\(k'\\) for the one that maximizes the right hand side of the Bellman equation. This method is referred to as grid search. While intuitive and straightforward, grid search has inherent limitations. Given each state today, the optimal \\(k'\\) to choose will, in general, lie somewhere between grids of \\(k\\). For example, fixing a grid of \\(k'\\) (say 100 grids between 0 to 10), given a state \\(k\\), the optimal \\(k'\\) to choose might be, say, \\(3.875\\), which lies between the two grids we specify: \\(3.8\\) and \\(3.9\\). With the grid search method, \\(k'\\) will be assigned to be either one of the two and we will update the value function based on that. Note that we are losing precisions of our policy function and value function as a better guess shall be directly using \\(k' = 3.875\\). Poor precisions can cause quantitative biases and make the results of the model less reliable and robust.\nContinuous optimization methods offer a powerful alternative by allowing us to directly pinpoint the optimal value. But the advantages of continuous optimization go beyond mere precision. These methods exploit sophisticated numerical properties and algorithms that significantly accelerate the computation process. For example, techniques such as gradient descent, Newton-Raphson, and quasi-Newton methods are designed to efficiently navigate the complex landscape of the optimization problem, converging to the optimal solution much faster than traditional methods.\nIn the realm of economics, many models feature continuous choice variables, such as investment, consumption and labor supply. Continuous optimization methods are particularly well-suited for these models, as they can seamlessly handle the continuous nature of the decision variables, providing more accurate and reliable results. In these scenarios, continuous methods are usually the go-to method.\nHowever, it is important to recognize that continuous optimization methods come with their own set of challenges. These methods often rely on assumptions about the smoothness and differentiability of the objective function, which may not always hold in practice. Additionally, they can be sensitive to the initial conditions and parameter choices, sometimes leading to convergence issues or local optima traps. As we delve deeper into these methods, we will explore strategies to mitigate these challenges, ensuring that we can harness their full potential.\n\n1. A simple root-finding problem\nYou might or might not have heard or used Newton-Raphson method before. Named after Isaac Newton and Joseph Raphson, it is probably the most famous root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.\nThe idea of it is very simple: we start with an initial guess, then to approximate the function by its tangent line, and finally to compute the x-intercept of this tangent line. This will often be a better approximation to the original function’s root than the first guess, and the method can be iterated.\nThe tangent line to the curve \\(f(x)\\) at \\(x = x_n\\) intercepts the x-axis at \\(x_{n+1}\\), then by definition the slope is \\[\nf'(x_n) = \\frac{f(x_n) - 0}{x_n - x_{n+1}}\n\\]\nSolving for \\(x_{n+1}\\) gets us, \\[\nx_{n+1}= x_n -  \\frac{f(x_n)}{f'(x_n)}\n\\]\nStart with some initial guess \\(x_0\\), we can iterate until convergence, where we obtain the root.\nBut what does this have to do with optimization? Root finding and optimization problems are actually of great mathematical resemblance. Think of the first thing you do with a (well-behaved) optimization problem. You take first order conditions! You get a function, say \\(g'(x) = 0\\), which is exactly a root-finding problem.\nNow, before resorting to any pre-written solvers, let’s do this very simple algorithm by hand to see how it works and get a sense of success before becoming hand-wavy and let solves do our jobs later on.\n\nimport Pkg\nPkg.activate(\".\")  # Activate a local environment \n\n\nusing Plots # add the Plots package if you don't have it yet.\nusing LinearAlgebra\n\nThe goal is to find the root of a function \\(f(x, p) = 0\\), where \\(x\\) is the independent variable and \\(p\\) are some parameters. We will consider where \\(p\\) is some general vectors.\n\n# A simple function of one variable and one parameter\nf(x, p) = -(x .- p) .^ 2;\n\nx = range(-5, 10, length=100);\nfx = f(x, 3);\nplot(x, fx, label = \"f(x)\")\nplot!(x, zeros(size(x)), label = \"x-axis\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously the function has a single root that equals to \\(p\\) (here 3).\nLet’s write down the Newton method to do this.\nHere this is just illustrative but for good practices of code, you shall always avoid using global variables and wrap the parameters into a struct (we have seen in lecture 2) or using a named tuple (defined below). Here due to the simplicity of our problem, this is not really a necessity.\n\n\"\"\"\nCreate an instance of the model, stored as a named tuple.\n\n\"\"\"\nfunction create_showcase_model(; xmin = -10.0, #beware of type\n                                 xmax = 10.0,\n                                 nx = 100,\n                                 p = [3.0, 4.0])\n        x_grid = LinRange(xmin, xmax, nx)\n        f(x, p) = - (x .- p) .^ 2  #you can define functions in named tuples\n        return (; x_grid, f, p, nx)\nend\n\ncreate_showcase_model\n\n\n\n# create the tuple \nmodel = create_showcase_model()\n\n# want to change some parameter? You can do create_showcase_model(p = 4.0)\n\n(x_grid = LinRange{Float64}(-10.0, 10.0, 100), f = var\"#f#20\"(), p = [3.0, 4.0], nx = 100)\n\n\n\n# calling something from the tuple is exactly same as struct\nmodel.p\n\n2-element Vector{Float64}:\n 3.0\n 4.0\n\n\nThe key here is for Newton Method we need analytical or numerical derivative of the function. For functions (or systems) that have an analytical form, you can automatically compute the first derivative by using the ForwardDiff package. You can think of what the package does is to use the known differential laws (like the chain rule) to efficiently solve for analytical derivatives directly. It really works like magic.\n    fp = x -&gt; f(x,p) # change f into a one input funcion\n    autodiff(fp) = x -&gt; ForwardDiff.jacobian(fp, x) # automatic differentiation of f on x\n(Note: For a single output function, simply use ForwardDiff.derivative. Here since I made f multi-variable. I use ForwardDiff.jacobian instead. )\nThen I use fp and autodiff(fp) to get the Jacobian at our guesses and update it with the standard Newton method rule.\n\n# Import the package to perform automatic differentiation (you might need to Pkg.add this)\nusing ForwardDiff\n\n\nfunction Newton(f;p = model.p, tol = 1e-8, maxiter = 1000)\n    x0 = zeros(eltype(p), length(p)) #initial guesses at 0.0\n    x1 = similar(x0)\n    fp = x -&gt; f(x,p) # change f into a one input funcion\n    # automatic differentiation of f\n    autodiff(fp) = x -&gt; ForwardDiff.jacobian(fp, x)\n    \n    iter = 0\n    error = 1.0\n    while (error &gt; tol && iter &lt; maxiter)\n        # not the most efficient but clear\n        J = autodiff(fp)(x0) \n        x1 = x0 .- J\\fp(x0) #or use a linear solver\n        error = maximum(abs.(x1 - x0))\n        iter += 1\n        x0 .= x1\n    end\n\n    return x1\nend\n\nNewton (generic function with 1 method)\n\n\n\nNewton(model.f)\n\n2-element Vector{Float64}:\n 2.9999999944120646\n 3.9999999925494194\n\n\n\nusing BenchmarkTools # package for seeing system details of code running\n\n\n@benchmark Newton(model.f)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  56.542 μs …  2.575 ms  ┊ GC (min … max): 0.00% … 95.74%\n Time  (median):     58.250 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   61.125 μs ± 70.074 μs  ┊ GC (mean ± σ):  3.18% ±  2.71%\n     ▄▆██▁                                                     \n  ▂▄▆█████▆▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂\n  56.5 μs         Histogram: frequency by time          72 μs &lt;\n Memory estimate: 36.55 KiB, allocs estimate: 672.\n\n\n\nThis is an idea of what is behind the nonlinear solvers of Julia. Of course they further optimized the code to make it more efficient.\nNow we use the NLsolve package. The naive implementation of it uses numerical Jacobian by finite differencing. The idea is to use the first order Taylor expansion to approximate the derivative. Note that here it does not require the function to have an analytical derivative.\n\nusing NLsolve\n\n\np = [3.0, 4.0]\nfp = x -&gt; model.f(x,p) # change f into a one input funcion\n# Newton method with numerical Jacobian by finite differencing\nnlsolve(fp, [0.0, 0.0]) #[0.0, 0.0] is the initial guess\n\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.0, 0.0]\n * Zero: [2.999945851229831, 3.9999278016397692]\n * Inf-norm of residuals: 0.000000\n * Iterations: 19\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 20\n * Jacobian Calls (df/dx): 20\n\n\n\n@benchmark nlsolve(fp, [0.0, 0.0]) \n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  35.917 μs …  2.868 ms  ┊ GC (min … max): 0.00% … 97.37%\n Time  (median):     37.083 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   39.754 μs ± 78.867 μs  ┊ GC (mean ± σ):  5.54% ±  2.75%\n       ▃█▄█▂▆▁▄▂                                               \n  ▁▁▂▄▅█████████▅▅▃▄▃▃▃▂▃▃▃▂▃▃▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  35.9 μs         Histogram: frequency by time        42.2 μs &lt;\n Memory estimate: 38.47 KiB, allocs estimate: 518.\n\n\n\nAs you can see, even with numerical differencing, it takes less than half of the time of the version I wrote down.\nHow about we also use auto differentiation in NLsolve?\n\n# since we have analytical f here, we can use autodiff\n@benchmark nlsolve(fp, [0.0, 0.0], autodiff = :forward)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  21.750 μs …  3.087 ms  ┊ GC (min … max): 0.00% … 97.40%\n Time  (median):     22.855 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   24.807 μs ± 58.908 μs  ┊ GC (mean ± σ):  4.68% ±  1.96%\n    █▆                                                         \n  ▂▅███▇▆▆▄▄▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂ ▃\n  21.8 μs         Histogram: frequency by time          35 μs &lt;\n Memory estimate: 20.77 KiB, allocs estimate: 258.\n\n\n\nNote that auto-differentiation provides better performance (more than 30% here).\nIf you like algebra, you can also manually inputting the derivatives:\n\nfprime(x, p) =  -2.0 * x .+ 2.0 .* p # analytical derivative\np = [3.0, 4.0]\nfprime_p = x -&gt; Diagonal(fprime(x, p)) #to create correct Jacobian\n\n#29 (generic function with 1 method)\n\n\n\nnlsolve(fp, fprime_p, [0.0, 0.0])\n\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.0, 0.0]\n * Zero: [2.9999458512298305, 3.9999278016397737]\n * Inf-norm of residuals: 0.000000\n * Iterations: 19\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 20\n * Jacobian Calls (df/dx): 20\n\n\n\n@benchmark nlsolve(fp, fprime_p, [0.0, 0.0])\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  20.292 μs …  3.155 ms  ┊ GC (min … max): 0.00% … 97.59%\n Time  (median):     21.250 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   23.250 μs ± 61.106 μs  ┊ GC (mean ± σ):  5.17% ±  1.96%\n    ▅█▆                                                        \n  ▃▆███▆▆▅▄▅▄▃▃▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂ ▃\n  20.3 μs         Histogram: frequency by time        33.1 μs &lt;\n Memory estimate: 18.34 KiB, allocs estimate: 255.\n\n\n\nSurprisingly (at least to me), manually inputting the Jacobian is not faster than automatic differentiation. This might not be the case when the non-linear system becomes more complicated but in such cases you won’t want to compute the analytical Jacobian anyways.\n*** Caveat: use auto-differentiation whenever possible. ***\nAn alternative package for root-finding is Nonlinearsolve, however from my experience it is slower than NLsolve.\n\n\n2. Interpolations\n\n2.1 Introduction to Interpolations in Julia\nAt its core, interpolation is the process of constructing new data points within the range of a discrete set of known data points. Suppose we have a set of \\(n\\) data points \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\), where \\(x_i\\) are the (ordered) independent variable values and \\(y_i\\) are the dependent variable values. Interpolation seeks to estimate the value of the function \\(f(x)\\) at any given point \\(x\\) within the interval \\([x_1, x_n]\\).\nThere are many interpolation methods, but the simplest yet powerful one is Linear Interpolation. The idea is to simply connect the dots and use the lines between as the original functions. Mathematically, \\[\n\\hat{f}(x) = y_i + \\frac{y_{i+1} - y_i}{x_{i+1} - x_i} (x - x_i)\n\\]\nLet’s give it a try! The standard Julia package to use is Interpolations.jl (for linear interpolation it is pretty straightforward to write down your own function, try that.)\n\n# Import the package to perform automatic differentiation (you might need to Pkg.add this)\nusing Interpolations\n\n\n# log function\nu1(c) = log(c)\n\nu1 (generic function with 1 method)\n\n\n\n# specify two linear grids, one with more points\nc_grid_lin_coarse = LinRange(0.1, 5.0, 10)\nc_grid_lin_fine = LinRange(0.1, 5.0, 100)\n\n100-element LinRange{Float64, Int64}:\n 0.1, 0.149495, 0.19899, 0.248485, 0.29798, …, 4.85152, 4.90101, 4.95051, 5.0\n\n\n\nu1_lin_coarse = linear_interpolation(c_grid_lin_coarse, u1.(c_grid_lin_coarse));\nu1_lin_fine = linear_interpolation(c_grid_lin_fine, u1.(c_grid_lin_fine));\n\nWe are done. Not hard, huh? You can think of u1_lin_coarse and u1_lin_fine as the constructed functions, but beware that its actual type is a wrapper of methods. To call it, simply do\n\nu1_lin_fine(2.723) # estimated function value at 2.723\n\n1.0017334325350888\n\n\nLet’s see how they perform.\nInterpolations are widely applied in economics is to estimate numerical functions. Remember we stored the value function \\(V(k)\\) on a discrete grid of \\(k\\). What if we need to find points that are not exactly on the grid points? Since we don’t know the exact functional form of \\(V\\).\n\n# check the accuracy of the interpolations\nc_grid_lin_finer = LinRange(0.1, 5.0, 1000)\nplot(c_grid_lin_finer, u1.(c_grid_lin_finer), label = \"original\")\nplot!(c_grid_lin_finer, u1_lin_fine.(c_grid_lin_finer), label = \"linear interpolation, finer grids\")\nplot!(c_grid_lin_finer, u1_lin_coarse.(c_grid_lin_finer), label = \"linear interpolation, coarser grids\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the graph, it is not hard to see that when the curvature is bigger, interpolating on the coarser grid gives visible numerical errors, while the finer grids perform better.\nWe can also do higher orders of Spline Interpolations. It allows you to get better functional approximation while making sure that the specified grid points are exactly reached.\nTake cubic spline interpolations as example. Mathematically, it constructs a cubic polynomial \\(S_i(x)\\) and finds \\(a_i, b_i ,c_i\\) such that\n\\[\nS_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3,\n\\] and \\(S_i(x_i) = y_i\\), \\(S_i(x_{i+1}) = y_{i+1}\\) and the first and second derivatives of \\(S_i(x)\\) are continuous at each \\(x_i\\).\nNote that from the second condition, you see that higher-order interpolations are also powerful in obtaining numerical derivatives. Recall from the graph before, the function constructed from linear interpolation is continuous but generally not differentiable everywhere due to kinks at each grid points.\nLet’s see an example with cubic spline interpolation and compare it with the linear interpolation on the coarser grid.\n\nu1_cubic_coarse = cubic_spline_interpolation(c_grid_lin_coarse, u1.(c_grid_lin_coarse));\n\n\nplot(c_grid_lin_finer, u1.(c_grid_lin_finer), label = \"original\")\nplot!(c_grid_lin_finer, u1_lin_coarse.(c_grid_lin_finer), label = \"linear interpolation, coarser grids\")\nplot!(c_grid_lin_finer, u1_cubic_coarse.(c_grid_lin_finer), label = \"cubic spline interpolation, coarser grids\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see that cubic spline interpolation (green line) gives smaller errors compared to linear interpolations (red line).\nOf course, we are still left with plenty of errors and in reality you will want to use more grids for this.\n\n\n2.2 Peril of Interpolations\nNow let’s try one of economists’ favorite function, the CRRA utility function (log is a special case) and do the same as before.\n\n# the constant σ is the relative risk aversion\nu2(c, σ) = c^(1.0 - σ)/(1.0 - σ)\n\nu2 (generic function with 1 method)\n\n\n\nσ = 2.0 # let the risk aversion be 2\nu2_lin_coarse = linear_interpolation(c_grid_lin_coarse, u2.(c_grid_lin_coarse, Ref(σ))); # \"Ref()\" specifies that σ is a scalar and won't be broadcasted\nu2_cubic_coarse = cubic_spline_interpolation(c_grid_lin_coarse, u2.(c_grid_lin_coarse, Ref(σ)));\n\n\nu2_cubic_coarse(2.5)\n\n-0.41019260074593517\n\n\n\nplot(c_grid_lin_finer, u2.(c_grid_lin_finer, Ref(σ)), label = \"original\")\nplot!(c_grid_lin_finer, u2_lin_coarse.(c_grid_lin_finer), label = \"linear interpolation, coarser grids\")\nplot!(c_grid_lin_finer, u2_cubic_coarse.(c_grid_lin_finer), label = \"cubic spline interpolation, coarser grids\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink:\n\nFor both interpolations, when do they have larger errors and why (hint: what feature does \\(u''(c)\\) have?)?\nFor both interpolations, when does cubic spline interpolation has a larger error and why (hint: what feature does \\(S_i(x)\\) have?)?\n\nExercise:\n\nChange \\(\\sigma\\) to 5 and 10 and do the same experiment. Try with both coarser and finer grids. How does the accuracy vary and what are the lessons?\n\n\n\n\n3. Introduction to Continuous Optimization\nImplementing optimization routines is simple in Julia with the help of nicely written packages. The hard part is to know to use the right method at the right time. There are hundreds of different routines out there and each is best suited for particular cases. Do you know the explicit form of the function? Is the function continuous? Is it concave/convex? Is it differentiable? Are there local maxes/mins? A big chunk of the machine/deep learning literature dedicates to this. The details of the topic is too broad to be covered by this short introduction but just keep in mind that there is no silver bullet of it.\n\n3.1 Unconstrained Optimization\nTake the function we defined in section 1: \\(f(x, p) = -(x - p) ^ 2\\).\nGiven \\(p\\), we want to find the maximum of the function. Obviously the correct result should again be \\(p\\).\nThe standard package for optimization to use in Julia is Optim.jl, but you might also want to check NLopt.jl and JuMP.jl. They each contain different methods and tackle different problems.\nFor the optimization routine, we will use LBFGS here, which is a quasi-Newton method. There are a bunch of other options. Check here for more info.\n\nusing Optim # you have to add this package\n\n\nf(x, p) = -(x[1] - p) ^ 2 #optimize take in vectors, here needs to specify x[1] for the first entry\nresult = optimize(x -&gt; -f(x, 3.0), [0.0], LBFGS()) # set p to 3, need to negative as we are trying to maximize the function\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.502828e-21\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 3.00e+00 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.00e+00 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.00e+00 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 5.99e+21 ≰ 0.0e+00\n    |g(x)|                 = 7.75e-11 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    1\n    f(x) calls:    3\n    ∇f(x) calls:   3\n\n\n\n# get the argmax\nresult.minimizer \n\n1-element Vector{Float64}:\n 3.0000000000387663\n\n\n\n# the abosulte error is\nabs(result.minimizer[1] - 3.0)\n\n3.8766323484651366e-11\n\n\nAgain here we can use Autodiff as we have a differentiable function with explicit form.\n\nresult = optimize(x -&gt; -f(x, 3.0), [0.0], LBFGS(); autodiff = :forward)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     0.000000e+00\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 3.00e+00 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.00e+00 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.00e+00 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = Inf ≰ 0.0e+00\n    |g(x)|                 = 0.00e+00 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    1\n    f(x) calls:    3\n    ∇f(x) calls:   3\n\n\n\nabs(result.minimizer[1] - 3.0)\n\n0.0\n\n\nNote that the accuracy improved by using automatic differentiation.\n\n\n3.2 Constrained Optimization\nI rarely use quotes but here is a good one.\n“In economic theory, an agent is a constrained optimization problem. A model consists of a collection of constrained optimization problems. Theories of general equilibrium, games, and macroeconomics acquire power by deploying an equilibrium concept whose role is to organize disparate choice problems by casting them within a coherent environment.”\n– Thomas Sargent\nLet’s maximize the same function \\(f(x,3)\\), but this time put an interval constraint on \\(x\\), we will try \\([2, 6]\\) (which include the global max) and \\([-2, 2]\\) (which does not include the global max).\nThe syntax for optimization with a bounded interval is as follows\noptimize(f, lower, upper, method; kwargs...)\nI will use the Brent() method for illustration. An alternative is to use GoldenSection().\n\noptimize(x -&gt; -f(x, 3.0), 2, 6, Brent())\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [2.000000, 6.000000]\n * Minimizer: 3.000000e+00\n * Minimum: 0.000000e+00\n * Iterations: 5\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 6\n\n\n\noptimize(x -&gt; -f(x, 3.0), -2, 2, Brent())\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [-2.000000, 2.000000]\n * Minimizer: 2.000000e+00\n * Minimum: 1.000000e+00\n * Iterations: 37\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 38\n\n\nThe Brent method gives correct answers in both cases, showing its robustness (and very fast). Beware some Quasi-Newton methods can fail in the second case as we do not reach \\(f'(x) = 0\\) in the interval.\n\n\n3.3 Optimize an Interpolated Function\nWe have talked about the importance of continuos optimization. However, sometimes we don’t know the exact functional form of which we are optimizing. Moreover, we might only know the value of our objective function at certain points. The example as you have seen in the last lecture, is the right hand side of the Bellman equation.\nThis is where interpolation twines with continuous optimization. From the functional value at discrete grids, we first estimate the function by interpolation, then optimize it using continuous optimization. This is a standard routine in solving many economic models.\nAgain just for showcasing, we will maximize the same function, but let’s pretend we only know its values at a discrete grid and need to approximate it with interpolations.\n\nlingrid = LinRange(2.0, 6.0, 50);\nf_val = -f.(lingrid, Ref(3.0)); # values of fx that we know\n# need to extrapolate as unconstrained optimization requires value out of bounds\n# consult the documentation of Interpolations.jl for details on extrapolations\nf_interp = linear_interpolation(lingrid, f_val, extrapolation_bc = Line());\n\nLet’s check how well the interpolation does.\n\nfiner_grid = LinRange(2.0, 6.0, 500);\nplot(finer_grid, -f.(finer_grid, Ref(3.0)), label = \"Original\")\nplot!(finer_grid, f_interp.(finer_grid), label = \"Linear interpolation\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two lines seem to align perfectly, so we are good to go, right?\nWe can do unconstrained and constrained optimization as before. Although we can’t use automatic differentiation anymore.\n\nresult1 = optimize(x -&gt; f_interp(x[1]), [4.0], LBFGS())\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     4.168613e-04\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 1.23e-05 ≰ 0.0e+00\n    |x - x'|/|x'|          = 4.14e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 3.65e-08 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 8.76e-05 ≰ 0.0e+00\n    |g(x)|                 = 6.58e-13 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    5\n    f(x) calls:    20\n    ∇f(x) calls:   20\n\n\n\nresult1.minimizer\n\n1-element Vector{Float64}:\n 2.979600858153335\n\n\n\nresult2 = optimize(f_interp, 2.0, 6.0, Brent())\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [2.000000, 6.000000]\n * Minimizer: 2.979592e+00\n * Minimum: 4.164937e-04\n * Iterations: 32\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 33\n\n\n\nresult2.minimizer\n\n2.979591849606776\n\n\nPretty big errors of 0.02 (this can be big, say you miss the equilibrium interest rate by 2%)! How come?\nThink about this for a bit yourself before reading forward.\nRemember when using the original function, we obtained error that was close to 0. We didn’t do anything different for the optimization routine, so it has to do with interpolation.\nGo back to the thinking question in the last section and look at the graph of the function again. You shall be able to find the answer yourself.\nIf not, let’s zoom in near the true maximization point (p):\n\nfiner_grid_shrinked = LinRange(2.8, 3.2, 500);\nplot(finer_grid_shrinked, f.(finer_grid_shrinked, Ref(3.0)), label = \"Original\")\nplot!(finer_grid_shrinked, -f_interp.(finer_grid_shrinked), label = \"Linear interpolation\")\nvline!([result2.minimizer], label = \"x obtained from the minimizer\", legend=:bottomright)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour should wow when you see this the first time.\nIn fact, the optimization package did exactly correct. It got the exact maximizer from your interpolated function. It is simply that the interpolated function is too biased around the maximizer due to high curvature around it.\nAlways keep in mind the peril of interpolation. It will save you enormous amount of time.\nHow to tackle this? More grid points and higher order of interpolations can help. Alternatively, you can put more points near where the curvature is high.\nSpecify the following grid where there are 10 more points in the range of 2 to 4 and 10 less points from 4 to 6. Number of total points remain unchanged.\n\nlingrid = vcat(collect(LinRange(2.0, 4.0, 35)), collect(LinRange(4.01, 6.0, 15))); #turn two linrange into vectors and combine them\nf_val = -f.(lingrid, Ref(3.0)); # values of fx that we know\nf_interp = linear_interpolation(lingrid, f_val, extrapolation_bc = false);\n\n\nresult2 = optimize(f_interp, 2.0, 6.0, Brent())\n\nResults of Optimization Algorithm\n * Algorithm: Brent's Method\n * Search Interval: [2.000000, 6.000000]\n * Minimizer: 3.000000e+00\n * Minimum: 9.262795e-10\n * Iterations: 20\n * Convergence: max(|x - x_upper|, |x - x_lower|) &lt;= 2*(1.5e-08*|x|+2.2e-16): true\n * Objective Function Calls: 21\n\n\n\nresult2.minimizer\n\n2.9999999842532477\n\n\n\nfiner_grid_shrinked = LinRange(2.8, 3.2, 500);\nplot(finer_grid_shrinked, f.(finer_grid_shrinked, Ref(3.0)), label = \"Original\")\nplot!(finer_grid_shrinked, -f_interp.(finer_grid_shrinked), label = \"Linear interpolation\")\nvline!([result2.minimizer], label = \"x obtained from the minimizer\", legend=:bottomright)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch better results. This kind of tricks is widely used in economic applications. In practice, people often use power grids instead of linear grids.\nIn the homework, you will rewrite the neo-classical growth model you have seen in lecture 2 with continuous optimization. You will compare the performances and results."
  }
]